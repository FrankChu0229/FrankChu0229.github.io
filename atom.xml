<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Frank Chu</title>
  <icon>https://www.gravatar.com/avatar/7c04e9129318ea2b45bdf06f71529116</icon>
  <subtitle>The world need dreamers who do.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://frankchu0229.github.io/"/>
  <updated>2017-10-30T03:43:18.000Z</updated>
  <id>http://frankchu0229.github.io/</id>
  
  <author>
    <name>Frank Chu</name>
    <email>chushb@shanghaitech.edu.cn</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>TF Symposium 1.4.0 Summary</title>
    <link href="http://frankchu0229.github.io/2017/10/25/tf_symposium/"/>
    <id>http://frankchu0229.github.io/2017/10/25/tf_symposium/</id>
    <published>2017-10-25T08:15:23.000Z</published>
    <updated>2017-10-30T03:43:18.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TF-Symposium-Summary"><a href="#TF-Symposium-Summary" class="headerlink" title="TF Symposium Summary"></a>TF Symposium Summary</h1><h2 id="L2L-with-进化算法-by-YiFei-Feng"><a href="#L2L-with-进化算法-by-YiFei-Feng" class="headerlink" title="L2L with 进化算法 by YiFei Feng"></a>L2L with 进化算法 by YiFei Feng</h2><ul><li>循环网络： 控制器； </li><li>evolution algo： 淘汰 -&gt; 变异</li><li>用简单的模型初始化</li><li>重复进化步骤</li></ul><h2 id="TF-与-生物学"><a href="#TF-与-生物学" class="headerlink" title="TF 与 生物学"></a>TF 与 生物学</h2><ul><li>生物大数据</li><li>机器学习问题： regression，classification</li><li>Deep Variant: inception-v3,将在第四季或明年开源</li></ul><h2 id="TF-性能-by-Frank-Chen"><a href="#TF-性能-by-Frank-Chen" class="headerlink" title="TF 性能 by Frank Chen"></a>TF 性能 by Frank Chen</h2><ul><li>出现性能问题： 了解问题， 用分析器：</li><li>Timeline TF自带分析器, 用chrome打开profile文件， 查看瓶颈</li><li>复制内存：复制越多， tf运算图越慢</li><li>如何改进： 1. 优化输入管道， 使用流水线； 2. 优化模型计算， 使用fuzed batch-norm, 使得在GPU上更快；3. 数据格式， CPU，GPU数据格式； 4. 运算图， matcol； 5. 单GPU变量：变量放到一个GPU即可； 单机多GPU， 可以尝试使用GPU来作为变量ser ver；变量过大或者过多， 多GPU共享变量； 多机多GPU， 通过网络赋值内存，比单机慢；可以使用local CPU做cache； 高级分布式培训模式：在每一个GPU集群内部，使用一个GPU作为parameter server，在外部用cpu做cache。</li></ul><h2 id="TF-机器人应用-by-Pi-Chuan-Chang"><a href="#TF-机器人应用-by-Pi-Chuan-Chang" class="headerlink" title="TF 机器人应用 by Pi-Chuan Chang"></a>TF 机器人应用 by Pi-Chuan Chang</h2><ul><li>Challenges: safety, control, transfer</li><li>Minitaur: -&gt; robot platform</li><li>openAI Gym environemt interface -&gt; environment</li><li>agnet: 树莓派； 微控制器计算能力不足</li><li>ARChirecture: signal generator (sin() function -&gt; 周期性) + balanbce controller(Fully connected layers) -&gt; actions </li><li>transfer to real robot: system identification, env randomization</li></ul><h2 id="TF-高层API-by-Yifei-Feng"><a href="#TF-高层API-by-Yifei-Feng" class="headerlink" title="TF 高层API by Yifei Feng"></a>TF 高层API by Yifei Feng</h2><ul><li>Keras： 高层神经网络API， 默认使用tf作为后端</li><li>tf.keras: 自定义的tf后端，change: from keras -&gt; from tensorflow.keras</li><li><p>tf.layers and tf.keras.layers, 共享实现方式，基本上相同</p></li><li><p>Estimators: canned estimators, 实现好的模型</p></li><li>keras 模型 -&gt; model_to_estimator -&gt; estimator model</li></ul><h3 id="Distributed-TF"><a href="#Distributed-TF" class="headerlink" title="Distributed TF"></a>Distributed TF</h3><ul><li>tf的一个很大优势， dev summit 2017</li><li>estimators 可进行分布式执行</li><li>1.4 版本中找到</li></ul><h3 id="Advice"><a href="#Advice" class="headerlink" title="Advice"></a>Advice</h3><ul><li>使用可以使用的最高层的API</li><li>用tf.layers and tf.keras 编写自定义模型</li><li>分布式训练 Estimators， 大部分情况下最好的选择</li></ul><h3 id="TF-Serving"><a href="#TF-Serving" class="headerlink" title="TF Serving"></a>TF Serving</h3><ul><li>github.com/tensorflow/serving</li><li>c++ libraries ： tf模型保存/输出格式； 通用核心模式</li><li>tf serving binariesL开箱急用的最佳实践， Docker容器， K8s教程</li></ul><h3 id="get-it"><a href="#get-it" class="headerlink" title="get it"></a>get it</h3><ul><li>pip apt-get 安装</li><li>www.tf.org/serving</li></ul><h2 id="TF-Lite（On-Smart-Devices）"><a href="#TF-Lite（On-Smart-Devices）" class="headerlink" title="TF Lite（On Smart Devices）"></a>TF Lite（On Smart Devices）</h2><ul><li>offline running on small devices</li><li>low-bandwidth, latency, power</li><li>Chanllenges: bandwith memory computation cpus</li><li>Tf works well on large services, tf-lite works on small devices</li><li>small binary size, low-overhead, optimized set of kernels</li><li>four parts: intepreter (optimized for all devices: few dependencies, small library less than 300k, fast load time, static memory plan, but no control flow), OPs/ kernels (NEON on ARM, Float &amp; quantized, many kernels for mobile apps), model file format (flatbuffers) mmap, more efficient than protocol buffer ; Hardware Acceleration (e.g., Android: Neural Network API; IOS: Core ML); Neural Network API: Part of Android Framework, tries to use as much hardware as possible. </li></ul><h3 id="Release"><a href="#Release" class="headerlink" title="Release"></a>Release</h3><ul><li>Developer preview: C++ and JAVA API</li><li>TOCO Converter</li><li>A set of builtin ops</li><li>Demo applications</li><li>Example models</li><li>MobileNet(Float)</li></ul><h2 id="Teaching-Machines-to-Draw"><a href="#Teaching-Machines-to-Draw" class="headerlink" title="Teaching Machines to Draw"></a>Teaching Machines to Draw</h2><ul><li>sketch-RNN: K encoder -&gt; Z (Latent Space Vectors)-&gt; Decoder </li></ul>]]></content>
    
    <summary type="html">
    
      TF Symposium Summary
    
    </summary>
    
      <category term="tensorflow" scheme="http://frankchu0229.github.io/categories/tensorflow/"/>
    
      <category term="notes" scheme="http://frankchu0229.github.io/categories/tensorflow/notes/"/>
    
    
      <category term="notes" scheme="http://frankchu0229.github.io/tags/notes/"/>
    
      <category term="summary" scheme="http://frankchu0229.github.io/tags/summary/"/>
    
      <category term="tensorflow" scheme="http://frankchu0229.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Dive Into Python3 Notes Native Datatypes</title>
    <link href="http://frankchu0229.github.io/2017/10/19/Dive%20Into%20Python3%20Notes%20-%20Native%20Datatypes/"/>
    <id>http://frankchu0229.github.io/2017/10/19/Dive Into Python3 Notes - Native Datatypes/</id>
    <published>2017-10-19T08:15:23.000Z</published>
    <updated>2017-10-30T03:46:42.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Dive-Into-Python3-Notes-Native-Datatypes"><a href="#Dive-Into-Python3-Notes-Native-Datatypes" class="headerlink" title="Dive Into Python3 Notes - Native Datatypes"></a>Dive Into Python3 Notes - Native Datatypes</h1><p>在python中，任何value都有type，但是你不需要显式地声明value的类型。Python根据该变量第一次被赋值的不同来对该变量的类型做出推断。</p><p>Python中主要的数据类型有：</p><ul><li>Boolean： <code>True</code> or <code>False</code></li><li>Numbers： they can be integers，floats，fractions(1/3) and even complex numbers</li><li>Strings: an sequence of Unicode characters</li><li>Bytes and byte arrays: e.g., an jpeg image</li><li>List: ordered sequence of values</li><li>Set: <strong>unordered</strong> sequence of values</li><li>Tuple: ordered and <strong>immutable</strong> sequence of values</li><li>Dict: unordered key-value pairs</li></ul><h2 id="Number"><a href="#Number" class="headerlink" title="Number"></a>Number</h2><ul><li>use <code>type()</code> and <code>isinstance()</code> to get the datatype and judge</li><li>use <code>int(2.0)</code> and <code>float(1)</code>, <code>int(-2.5) = -2</code> 来做type转换</li><li>float numbers 小数点后最多只能有15位</li><li>与python2既有int又有long不同， python3中只有int， 并且int可以取得像python2中long的最大值</li></ul><h3 id="Number-Operations"><a href="#Number-Operations" class="headerlink" title="Number Operations"></a>Number Operations</h3><ul><li><code>/</code> 除法， 返回float类型</li><li><code>//</code> 整数除法： <code>11 // 2 = 5</code>, <code>-11 // 2 = -6</code> </li><li><code>**</code> 幂操作</li><li><code>%</code> 取余数</li></ul><h3 id="分数-Fractions"><a href="#分数-Fractions" class="headerlink" title="分数 Fractions"></a>分数 Fractions</h3><ul><li>使用 <code>import fractions</code>, <code>print(fractions.Fraction(1, 3))</code></li></ul><h3 id="其他数字"><a href="#其他数字" class="headerlink" title="其他数字"></a>其他数字</h3><ul><li>PI : <code>import math</code>, <code>print(math.pi)</code></li><li>需要注意的是python中没有infinite的精度， 因此像<code>sin(\pi)</code>的结果不是0， 而是一个很小的数</li></ul><h3 id="数字用在-bolean-context"><a href="#数字用在-bolean-context" class="headerlink" title="数字用在 bolean context"></a>数字用在 bolean context</h3><ul><li>如果数字的value为0， 则为False； 否则为True</li></ul><h2 id="List"><a href="#List" class="headerlink" title="List"></a>List</h2><p>Python中list大小可以动态改变，并且list中可以add各种不同类型的value。</p><h3 id="Define-a-list"><a href="#Define-a-list" class="headerlink" title="Define a list"></a>Define a list</h3><p>Define a list <code>a = []</code> or <code>a = [1, &#39;hello&#39;, 1.0, [1,2,3]]</code></p><h3 id="Get-the-values-from-the-list"><a href="#Get-the-values-from-the-list" class="headerlink" title="Get the values from the list"></a>Get the values from the list</h3><p>Get the values from the list: <code>a[0], a[1], a[2], a[3]</code> or <code>a[-1], a[-2], a[-3], a[-4]</code></p><h3 id="Slicing-the-list"><a href="#Slicing-the-list" class="headerlink" title="Slicing the list"></a>Slicing the list</h3><p>Slicing the list: <code>a[1:3]</code> can get the sublist of <code>a[0]</code> and <code>a[1]</code>, 即不包括最后的一位；如果前后有省略，则默认为从该list的第一位开始(包含)，或者到最后一位(包含)；如果前后都没有，则结果为整个list</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">a[:2] -&gt; [1, &apos;hello&apos;]</div><div class="line">a[2:] -&gt; [1.0, [1,2,3]]</div></pre></td></tr></table></figure><h3 id="Adding-items-to-list"><a href="#Adding-items-to-list" class="headerlink" title="Adding items to list:"></a>Adding items to list:</h3><p>Four ways to add items to a list:</p><ul><li>Operation <code>+</code> : <code>a = [1, 2]</code>, <code>b = [2]</code>, <code>a = a + b -&gt; [1,2,2]</code>, 这里需要注意的是， <code>+</code>操作会new 一个新的list</li><li><code>a.append()</code>函数, <code>append()</code>函数会把append的内容(无论什么类型)加到list中, 例如： <code>a.append([4,5,6]) -&gt; a = [1,2,2,[4,5,6]]</code></li><li><code>a.extend([4,5,6])</code>函数会把extend的内容一个一个的加到list中， 得到<code>a = [1,2,2,4,5,6]</code></li><li><code>a.insert(0, &#39;4&#39;)</code>, 会把value插入到指定的index处</li></ul><h3 id="Searching-in-a-list"><a href="#Searching-in-a-list" class="headerlink" title="Searching in a list"></a>Searching in a list</h3><ul><li><code>a_list.count(&#39;a&#39;)</code>: return the number of specific values in a list</li><li><code>&#39;a&#39; in a_list</code>, return a boolean value to show if the value is in the list</li><li><code>a_list.index(&#39;a&#39;)</code> to return the index of the given value</li></ul><h3 id="Removing-items-in-a-list"><a href="#Removing-items-in-a-list" class="headerlink" title="Removing items in a list"></a>Removing items in a list</h3><ul><li><code>del a_list[1]</code> to delete the value in the index of 1</li><li><code>a_list.remove(&#39;a&#39;)</code> to remove the first occurance of the value in a list</li><li><code>a_list.pop()</code> removes the last value in a list</li><li><code>a_list.pop(0)</code> removes the value in the given index</li></ul><h3 id="List-in-a-boolean-context"><a href="#List-in-a-boolean-context" class="headerlink" title="List in a boolean context"></a>List in a boolean context</h3><ul><li>empty list will be <code>False</code> in a boolean context and list which is not empty will be <code>True</code>.</li></ul><h2 id="Tuples"><a href="#Tuples" class="headerlink" title="Tuples"></a>Tuples</h2><p>A tuple is an immutable list. A tuple cannot be changed once it is created.</p><p>Tuple和List比较像， tuple可以看做是一个不可变的list。Tuple比list更快，如果这个set是不会被更改的，则使用Tuple，同时它也会使你的code更加安全。</p><p>在Python中，只有immutable的才可以作为Dict 的key，如果tuple中的变量都是immutable的，那么</p><h3 id="Tuple的创建和获取元素"><a href="#Tuple的创建和获取元素" class="headerlink" title="Tuple的创建和获取元素"></a>Tuple的创建和获取元素</h3><ul><li><code>a_tuple = (1,2,3,Ture)</code>, Tuple用圆括号来创建，元素获取的方式同list</li><li>Tuple的slice方式<code>a_list[1:3]</code>同list，不同之处在于，tuple的slice返回的仍是tuple</li></ul><h3 id="Tuple中元素的操作"><a href="#Tuple中元素的操作" class="headerlink" title="Tuple中元素的操作"></a>Tuple中元素的操作</h3><ul><li>Tuple元素的操作不包括：如 <code>pop()</code>, <code>append()</code>等添加或者删除元素的函数</li><li>Tuple中可以使用<code>count()</code>和<code>index()</code>, <code>&#39;a&#39; in a_tuple</code>等操作 </li></ul><h3 id="Tuple-和-List的转换"><a href="#Tuple-和-List的转换" class="headerlink" title="Tuple 和 List的转换"></a>Tuple 和 List的转换</h3><ul><li><code>tuple()</code> functuon can convert list into tuples and <code>list()</code> function can convert tuples into list. </li></ul><h3 id="Tuples-in-boolean-context"><a href="#Tuples-in-boolean-context" class="headerlink" title="Tuples in boolean context"></a>Tuples in boolean context</h3><ul><li>同list一样， 在boolean context中，如果tuple为空，则返回False； 否则返回True</li><li>值得注意的是，在创建只有一个值的tuple时，应为<code>(False,)</code>；即需要加一个逗号”，“， 否则python无法识别出是带有一个值的tuple。</li></ul><h3 id="将tuple用来同时给多个变量赋值"><a href="#将tuple用来同时给多个变量赋值" class="headerlink" title="将tuple用来同时给多个变量赋值"></a>将tuple用来同时给多个变量赋值</h3><ul><li><code>(x, y, z) = (1,2,3)</code></li></ul><h2 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h2><p>在python中， set是无序的，并且没有冗余元素的数据结构。A single set can contain values of any immutable datatype, 因此像list不能是set中的元素，因为list不是immutable的，但是tuple可以。</p><h3 id="Set的创建"><a href="#Set的创建" class="headerlink" title="Set的创建"></a>Set的创建</h3><ul><li><code>a_set = {1,2,3}</code> 可以用来创建一个set， 但是<code>{}</code>为一个空的dict； <code>set()</code>才是创建一个空set的方式。</li><li>同时也可以使用<code>set([1,2,3])</code>函数来将一个list转换为一个set，此操作不会对list做改变。</li></ul><h3 id="增加和删除set中的元素"><a href="#增加和删除set中的元素" class="headerlink" title="增加和删除set中的元素"></a>增加和删除set中的元素</h3><h4 id="增加set中的元素："><a href="#增加set中的元素：" class="headerlink" title="增加set中的元素："></a>增加set中的元素：</h4><ul><li><code>a_set.add({1,2,3})</code>, 同list中的<code>append()</code>一样， 会将传进去的datatype加到一个变量中去</li><li><code>a_set.update({1,2,3}，{1，2，3})</code>, 同list中的<code>extend()</code>一样， 会将set或者list中的每一个元素加到set中，并且会去除重复元素，保证unique。</li></ul><h4 id="删除set中的元素："><a href="#删除set中的元素：" class="headerlink" title="删除set中的元素："></a>删除set中的元素：</h4><ul><li><code>a_set.discard(1)</code> 将会删除set中的变量1，如果变量1不存在，没有任何影响；</li><li><code>a_set.remove(1)</code>将会删除set中的变量1，如果变量1不存在，则会raise Exception KeyError。</li><li><code>a_set.pop()</code> 将会随机删除set中的一个元素，因为set是无序的，因此不能像list一样pop掉最后的元素。</li><li><code>a_set.clear()</code> 将会clear掉set的全部元素，返回一个空的set</li></ul><h3 id="Set的操作"><a href="#Set的操作" class="headerlink" title="Set的操作"></a>Set的操作</h3><ul><li><code>1 in a_set</code> will return True if a_set contains 1, otherwise return False</li><li><code>a_set.union(b_set)</code> 将会把 a_set和b_set取并集</li><li><code>a_set.intersection(b_set)</code> 将会取a_set 和 b_set 的交集</li><li><code>a_set.difference(b_set)</code> 将会取在a_set 但是不在b_set中的元素</li><li><code>a_set.symmetric_difference(b_set)</code> 将会取只出现在a_set 和 只出现在b_set的并集</li></ul><h3 id="Set-in-a-boolean-context"><a href="#Set-in-a-boolean-context" class="headerlink" title="Set in a boolean context"></a>Set in a boolean context</h3><ul><li>同list 和tuple 一样，空的set在boolean context中为False，否则为True</li></ul><h2 id="Dictionary"><a href="#Dictionary" class="headerlink" title="Dictionary"></a>Dictionary</h2><p>在python中，Dictionary为无序的key-value pair的set。当向dict中添加一个key的时候，你也必须添加一个相应的值。</p><h3 id="Dict的创建和元素获取"><a href="#Dict的创建和元素获取" class="headerlink" title="Dict的创建和元素获取"></a>Dict的创建和元素获取</h3><ul><li><code>{}</code> 为空的dict， <code>a_dict = {&#39;1&#39;: &#39;h&#39;, &#39;2&#39;: &#39;w&#39;}</code> 创建一个dict</li><li><code>a_dict[&#39;1&#39;]</code> 根据相应的key，来获取对应的value； 当<code>a_dict[&#39;3&#39;]</code> 获取一个未包含的key的时候，会raise KeyError Exception</li></ul><h3 id="Dict的更改"><a href="#Dict的更改" class="headerlink" title="Dict的更改"></a>Dict的更改</h3><ul><li><code>a_dict[&#39;3&#39;] = &#39;hhh&#39;</code>来增加新的key-value pair</li><li><code>a_dict[&#39;1&#39;] = &#39;hh&#39;</code> 来对已经存在的key-value pair进行更改</li></ul><h3 id="Mixed-Value-Dict"><a href="#Mixed-Value-Dict" class="headerlink" title="Mixed Value Dict"></a>Mixed Value Dict</h3><p>Dict 的value可以是任何类型， 但是dict的key 需要时immutable的， 例如： integer， string， tuple等。</p><ul><li>和list、set、tuple等一样, dict可以用<code>len()</code>来返回key-value pair的个数</li><li>和list、set、tuple等一样， 可以使用<code>in</code> 来判断一个变量是不是dict中的一个key</li></ul><h3 id="Dict-in-a-boolean-context"><a href="#Dict-in-a-boolean-context" class="headerlink" title="Dict in a boolean context"></a>Dict in a boolean context</h3><ul><li>An empty dict is False</li><li>Otherwise, it’s True</li></ul><h2 id="None"><a href="#None" class="headerlink" title="None"></a>None</h2><ul><li>None 是Python中一个比较特殊的constants，它是一个null值。</li><li>None有他自己特殊的type，为NoneType</li><li>将None同其他不是None的值进行比较，都将返回False。</li><li>所有的None值 (值为None的变量) 都相等。</li></ul>]]></content>
    
    <summary type="html">
    
      Dive Into Python3 Notes - Native Datatypes
    
    </summary>
    
      <category term="python" scheme="http://frankchu0229.github.io/categories/python/"/>
    
      <category term="notes" scheme="http://frankchu0229.github.io/categories/python/notes/"/>
    
    
      <category term="notes" scheme="http://frankchu0229.github.io/tags/notes/"/>
    
      <category term="summary" scheme="http://frankchu0229.github.io/tags/summary/"/>
    
      <category term="Python" scheme="http://frankchu0229.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Dive Into Python3 Notes Your First Program</title>
    <link href="http://frankchu0229.github.io/2017/10/15/Dive%20Into%20Python%20Notes/"/>
    <id>http://frankchu0229.github.io/2017/10/15/Dive Into Python Notes/</id>
    <published>2017-10-15T08:15:23.000Z</published>
    <updated>2017-10-30T03:46:33.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Dive-Into-Python3-Notes-Your-First-Program"><a href="#Dive-Into-Python3-Notes-Your-First-Program" class="headerlink" title="Dive Into Python3 Notes - Your First Program"></a>Dive Into Python3 Notes - Your First Program</h1><h2 id="Define-Functions"><a href="#Define-Functions" class="headerlink" title="Define Functions"></a>Define Functions</h2><ul><li>python function中没有定义返回类型。 每一个python functiuon都会有一个返回值，如果function中含有return value， 则返回相应的值；否则返回<code>None</code>(Python中的null)</li><li>在python中， 变量(anything)从不显示声明类型，python 内部会自动track相应的类型，内部根据该变量第一次被赋予的类型来决定该变量的类型。</li><li>python function允许参数有默认值，如果该参数没有被输入，则使用该参数的默认值；另外，function参数的指定，可以不按照在function中定义的顺序，只要提供该function中的变量名字即可。</li></ul><h2 id="Writing-Readable-Code"><a href="#Writing-Readable-Code" class="headerlink" title="Writing Readable Code"></a>Writing Readable Code</h2><h3 id="Documentation-Strings-doc-string-for-short"><a href="#Documentation-Strings-doc-string-for-short" class="headerlink" title="Documentation Strings (doc-string for short)"></a>Documentation Strings (doc-string for short)</h3><ul><li>“”” “”” triple quotes signify a multi-line doc-string. </li><li>Doc-string 必须在相应函数(类)中最前的地方被定义，在Python中， everything is object， doc-string 可以通过该object的.__doc_string得到。</li></ul><h2 id="Python-Import-Path"><a href="#Python-Import-Path" class="headerlink" title="Python Import Path"></a>Python Import Path</h2><ul><li>在Python中，当你想import其他python module的时候，python会首先去几个地方进行查找。 Specifically, python会在<code>sys.path</code>中的所有目录下进行查找。</li><li><code>sys.path</code> 是一个list， 你可以像操作正常的list一样将你的python module加入到<code>sys.path</code>中，例如：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import sys</div><div class="line">sys.path.insert(0, &quot;./tmp/hello_world.py&quot;)</div></pre></td></tr></table></figure><h2 id="Everything-Is-Object-In-Python"><a href="#Everything-Is-Object-In-Python" class="headerlink" title="Everything Is Object In Python"></a>Everything Is Object In Python</h2><ul><li>Python module is a object, so when you want to use the public functions, classes or attributes in the module, you need to add the module name before that, e.g., <code>FirstProgram.HelloWorld()</code></li><li>Python function is also a object, so you can use <code>FunctionName.__doc__</code> to get its doc-string.</li><li>In python, <code>classes</code>, <code>functions</code>, <code>modules</code> are first-class objects, which means you can pass them as a parameter in a function.</li></ul><h2 id="Python-Indenting-Code"><a href="#Python-Indenting-Code" class="headerlink" title="Python Indenting Code"></a>Python Indenting Code</h2><ul><li>在python中，函数没有显式的begin、end标志，也没有像java等语言用{}来标记函数的start和end。Python中用<code>冒号:</code>和<code>缩进</code>。</li><li>可以这样简单的理解，像在java中，涉及到code block的地方，需要有{}来标记，例如if， for， while， function等；在python中对于这些部分需要有<code>:</code>和缩进</li></ul><h2 id="Exceptions-In-Python"><a href="#Exceptions-In-Python" class="headerlink" title="Exceptions In Python"></a>Exceptions In Python</h2><ul><li>If you’re opening a file, it might not exist. If you’re importing a module, it might not be installed. If you’re connecting to a database, it might be unavailable, or you might not have the correct security credentials to access it. If you know a line of code may raise an exception, you should handle the exception using a <code>try...except</code> block, and <code>raise</code> to generate exceptions.</li><li>例如：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">try: </div><div class="line">import chardet</div><div class="line">except ImportError:</div><div class="line">chardet = None</div></pre></td></tr></table></figure><h2 id="In-Python-everything-is-case-sensitive"><a href="#In-Python-everything-is-case-sensitive" class="headerlink" title="In Python, everything is case-sensitive"></a>In Python, everything is case-sensitive</h2><h2 id="Running-Scripts"><a href="#Running-Scripts" class="headerlink" title="Running Scripts"></a>Running Scripts</h2><p>Python modules are objects and you can use some attributes of them:</p><ul><li>all modules have an attribute: <code>__name__</code>. 如果你是import a module， <code>__name__</code>即为该module的filename (不带路径名)；但如果你将该module单独的去run，此时<code>__name__</code>的值为它的default值：<code>__main__</code></li></ul>]]></content>
    
    <summary type="html">
    
      Dive Into Python3 Notes - Your First Program
    
    </summary>
    
      <category term="python" scheme="http://frankchu0229.github.io/categories/python/"/>
    
    
      <category term="notes" scheme="http://frankchu0229.github.io/tags/notes/"/>
    
      <category term="summary" scheme="http://frankchu0229.github.io/tags/summary/"/>
    
      <category term="Python" scheme="http://frankchu0229.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>MySQL Summary</title>
    <link href="http://frankchu0229.github.io/2017/10/03/MySQLNotes/"/>
    <id>http://frankchu0229.github.io/2017/10/03/MySQLNotes/</id>
    <published>2017-10-03T13:24:57.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><p>Mysql 是最流行的关系型数据库管理系统，由瑞典MySQL公司开发，目前属于Oracle。MySQL使用标准的SQL数据语言形式。</p><h2 id="Concept-In-MySQL"><a href="#Concept-In-MySQL" class="headerlink" title="Concept In MySQL"></a>Concept In MySQL</h2><ul><li>主键： 主键是唯一的，一个数据表只能有一个主键， 可以用主键来查询数据。</li><li>外键：用于关联两个数据表。</li><li>复合键(组合键)：将多个列作为一个索引键，一般用于复合索引。 </li></ul><h2 id="MySQL-Install-and-Setup"><a href="#MySQL-Install-and-Setup" class="headerlink" title="MySQL Install and Setup"></a>MySQL Install and Setup</h2><ul><li><code>service mysqld start</code> 启动mysql</li><li><code>mysql -h localhost -u root -p</code> 进入mysql client端进行执行简单的SQL命令</li><li><code>quit</code> OR <code>Ctrl + D</code> 退出mysql。</li><li><code>mysqladmin -u root password &quot;new_password&quot;;</code> to set new password.</li><li><code>SHOW DATABASES;</code> to list all databases</li></ul><h2 id="MySQL-数据类型"><a href="#MySQL-数据类型" class="headerlink" title="MySQL 数据类型"></a>MySQL 数据类型</h2><p>MySQL中主要有三种数据类型：熟悉、日期|时间、字符串，MySQL支持所有标准SQL数值数据类型 <a href="http://www.cnblogs.com/zbseoag/archive/2013/03/19/2970004.html" target="_blank" rel="external">See more info.</a>。</p><h2 id="MySQL-数据库操作命令"><a href="#MySQL-数据库操作命令" class="headerlink" title="MySQL 数据库操作命令"></a>MySQL 数据库操作命令</h2><ul><li><code>SHOW DATABASES;</code> To list all databases</li><li><code>use XXX;</code> To choose the XXX database</li><li><code>SHOW TABLES;</code> 列出该数据库中所有表</li><li><code>SHOW COLUMNS FROM XX</code> OR <code>DESCRIBE XX;</code> 显示数据表的属性信息</li><li><code>SHOW INDEX FROM XX</code> 显示数据表的详细索引信息</li><li><code>SHOW TABLE STATUS FROM XXX</code> 显示数据库管理系统的性能及统计信息</li><li><code>SHOW TABLE STATUS FROM XXX LIKE &#39;runoob%&#39;</code> 显示以runoob开头的表的信息</li><li><code>SHOW TABLE STATUS FROM XXX LIKE &#39;runoob%&#39;\G</code> 显示以runoob开头的表的信息, 结果按照列打印</li><li><code>select version(),current_date();</code> 显示版本和日期， 可见mysql对大小写结果一致。也可多行语句， 直到见到”;”为止。</li></ul><h2 id="MySQL-常用操作命令"><a href="#MySQL-常用操作命令" class="headerlink" title="MySQL 常用操作命令"></a>MySQL 常用操作命令</h2><ul><li><code>create database XXX;</code> 创建数据库XXX</li><li><code>deop database XXX;</code> 删除数据库XXX</li><li><code>use XXX;</code> 选择数据库XXX</li><li><code>CREATE TABLE table_name(column_name column_type);</code> 创建数据表</li></ul><p>例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mysql&gt; create table test_table( id INT NOT NULL AUTO_INCREMENT, name VARCHAR(20) NOT NULL, sex CHAR(1) NOT NULL, birth DATE, birth_addr VARCHAR(20), PRIMARY KEY (id));</div></pre></td></tr></table></figure></p><ul><li><code>drop table XXX;</code> 删除数据表；</li><li><code>insert into table_name (filed1, ...fieldn 可省略) values (value1, value2, ...valuen)</code> 插入数据；</li></ul><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">insert into test_table values (1, &apos;john&apos;, &apos;m&apos;, &apos;1992-01-29&apos;, &apos;shanghai&apos;);</div></pre></td></tr></table></figure><ul><li><code>select * from table_name;</code>显示该表下的全部数据；</li></ul><p>MySQL 查询数据语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">SELECT column_name,column_name</div><div class="line">FROM table_name1, table_name2</div><div class="line">[WHERE Clause]</div><div class="line">[OFFSET M ][LIMIT N]</div></pre></td></tr></table></figure><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">select name,sex from test_table where id &gt; 1 limit 1;</div></pre></td></tr></table></figure><ul><li><code>update table_name set field1=new-value1, field2=new-value2</code> 更新数据</li></ul><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">update test_table set name=&apos;mike&apos; where id=1;</div></pre></td></tr></table></figure><ul><li><p><code>delete from table_name [where clause];</code>  删除表中数据，若没有指定条件，则删除整个表。</p></li><li><p><code>WHERE FIELD LIKE &#39;%XXX&#39;;</code> Like子句 + %，起到查询包含XXX的数据的左右， 例如： <code>select name from test_table where birth like &#39;1992%&#39;;</code></p></li><li><code>WHERE NAME REGEXP &#39;regex expression&#39;</code>, 例如： <code>SELECT name FROM person_tbl WHERE name REGEXP &#39;^[aeiou]|ok$&#39;;</code></li></ul><h3 id="ALTER-使用"><a href="#ALTER-使用" class="headerlink" title="ALTER 使用"></a>ALTER 使用</h3><ul><li><code>ALTER TABLE testalter_tbl  DROP i;</code> 删除一列</li><li><code>ALTER TABLE testalter_tbl ADD i INT;</code> 增加一列</li><li><code>ALTER TABLE testalter_tbl ADD i INT FIRST;</code> 在指定位置增加一列</li><li><code>ALTER TABLE testalter_tbl ADD i INT AFTER c;</code> 在指定位置增加一列</li><li><code>ALTER TABLE testalter_tbl MODIFY c CHAR(10);</code> 修改字段类型</li><li><code>ALTER TABLE testalter_tbl CHANGE j j INT;</code> 修改字段名以及类型</li><li><code>ALTER TABLE testalter_tbl   -&gt; MODIFY j BIGINT NOT NULL DEFAULT 100;</code> 设置默认值和NOT NULL</li><li><code>ALTER TABLE testalter_tbl ALTER i SET DEFAULT 1000;</code> 修改字段默认值</li><li><code>ALTER TABLE testalter_tbl ALTER i DROP DEFAULT;</code> 删除字段默认值</li><li><code>ALTER TABLE testalter_tbl RENAME TO alter_tbl;</code> 更改表名</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="http://blog.csdn.net/chinacodec/article/details/5797127/" target="_blank" rel="external">http://blog.csdn.net/chinacodec/article/details/5797127/</a></li><li><a href="http://www.runoob.com/mysql/mysql-create-database.html" target="_blank" rel="external">http://www.runoob.com/mysql/mysql-create-database.html</a></li><li><a href="http://www.runoob.com/java/java-mysql-connect.html" target="_blank" rel="external">http://www.runoob.com/java/java-mysql-connect.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      MySQL summary。
    
    </summary>
    
      <category term="summary" scheme="http://frankchu0229.github.io/categories/summary/"/>
    
    
      <category term="MySQL" scheme="http://frankchu0229.github.io/tags/MySQL/"/>
    
      <category term="notes" scheme="http://frankchu0229.github.io/tags/notes/"/>
    
  </entry>
  
  <entry>
    <title>Tmux Summary</title>
    <link href="http://frankchu0229.github.io/2017/09/23/TmuxNotes/"/>
    <id>http://frankchu0229.github.io/2017/09/23/TmuxNotes/</id>
    <published>2017-09-23T13:33:01.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tmux-Summary"><a href="#Tmux-Summary" class="headerlink" title="Tmux Summary"></a>Tmux Summary</h1><h2 id="翻页"><a href="#翻页" class="headerlink" title="翻页"></a>翻页</h2><p><code>Ctrl + B + [</code></p>]]></content>
    
    <summary type="html">
    
      Tmux Summary。
    
    </summary>
    
      <category term="summary" scheme="http://frankchu0229.github.io/categories/summary/"/>
    
    
      <category term="notes" scheme="http://frankchu0229.github.io/tags/notes/"/>
    
      <category term="linux" scheme="http://frankchu0229.github.io/tags/linux/"/>
    
      <category term="summary" scheme="http://frankchu0229.github.io/tags/summary/"/>
    
      <category term="tmux" scheme="http://frankchu0229.github.io/tags/tmux/"/>
    
  </entry>
  
  <entry>
    <title>Linux Shell Summary</title>
    <link href="http://frankchu0229.github.io/2017/09/03/linuxShellSummary/"/>
    <id>http://frankchu0229.github.io/2017/09/03/linuxShellSummary/</id>
    <published>2017-09-03T13:33:01.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linus-Shell-Summary"><a href="#Linus-Shell-Summary" class="headerlink" title="Linus Shell Summary"></a>Linus Shell Summary</h1><h2 id="查看端口使用情况："><a href="#查看端口使用情况：" class="headerlink" title="查看端口使用情况："></a>查看端口使用情况：</h2><p><code>sudo lsof -i:8888</code></p><p>若要停止使用这个端口的程序，使用kill +对应的pid即可</p><h2 id="Copy-Public-Key"><a href="#Copy-Public-Key" class="headerlink" title="Copy Public Key :"></a>Copy Public Key :</h2><p><code>ssh-agent</code> //启动 ssh-agent，大家可以自行搜索自己所使用OS启动 ssh-agent的方式。</p><p><code>ssh-add</code></p><p><code>ssh-add -l</code> // 检查自己的私钥是否被ssh-agent管理</p><p>执行以下命令，将本地public-key添加到的authorized_keys里面：</p><p><code>ssh-copy-id -i ~/.ssh/id_rsa.pub User@HostName</code></p><h1 id="查找gz文件中的log"><a href="#查找gz文件中的log" class="headerlink" title="查找gz文件中的log"></a>查找gz文件中的log</h1><p>zgrep zcat</p><ul><li><code>zgrep 60a3b7146b12 laindocker.log-20170506.gz &gt; ~/T1836-all</code></li></ul><h2 id="从服务器上-下载-上传-文件到本地"><a href="#从服务器上-下载-上传-文件到本地" class="headerlink" title="从服务器上 (下载|上传) 文件到本地"></a>从服务器上 (下载|上传) 文件到本地</h2><ul><li>下载： <code>scp xxx@gpu:/home/src /home/drc</code></li></ul><p>若是目录的话， 加 <code>-r</code></p><ul><li>上传： <code>scp -r /localpath xxx@gpu:remotePath</code></li></ul><h2 id="ubuntu-soft-link"><a href="#ubuntu-soft-link" class="headerlink" title="ubuntu soft link"></a>ubuntu soft link</h2><p>文件夹建立软链接（用绝对地址）</p><p>ln -s 源地址 目的地址</p><p>比如我把linux文件系统rootfs_dir软链接到/home/xxx/目录下</p><p>　　<code>ln -s /opt/linux/rootfs_dir  /home/xxx/rootfs_dir</code>　　</p><h2 id="查看系统版本"><a href="#查看系统版本" class="headerlink" title="查看系统版本"></a>查看系统版本</h2><p><code>cat /etc/issue</code>  </p><p>　　</p>]]></content>
    
    <summary type="html">
    
      Linux shell summary。
    
    </summary>
    
      <category term="summary" scheme="http://frankchu0229.github.io/categories/summary/"/>
    
    
      <category term="notes" scheme="http://frankchu0229.github.io/tags/notes/"/>
    
      <category term="linux" scheme="http://frankchu0229.github.io/tags/linux/"/>
    
      <category term="shell" scheme="http://frankchu0229.github.io/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>Git Summary</title>
    <link href="http://frankchu0229.github.io/2017/09/03/gitSummary/"/>
    <id>http://frankchu0229.github.io/2017/09/03/gitSummary/</id>
    <published>2017-09-03T13:24:57.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Git-summary"><a href="#Git-summary" class="headerlink" title="Git summary"></a>Git summary</h1><h2 id="git-reset-–hard"><a href="#git-reset-–hard" class="headerlink" title="git reset –hard"></a>git reset –hard</h2><p>未commit的chagne会随着branch的切换而移动，因此，当在某一个branch中<code>git reset --hard</code>的时候， changes就会消失。</p><h2 id="git-删除remote-branch"><a href="#git-删除remote-branch" class="headerlink" title="git 删除remote branch"></a>git 删除remote branch</h2><p><code>git push origin --delete branch_name</code></p><h2 id="git-rm-files-in-repo"><a href="#git-rm-files-in-repo" class="headerlink" title="git rm files in repo"></a>git rm files in repo</h2><p><code>git rm --cache files</code></p><h2 id="show-git-track-files"><a href="#show-git-track-files" class="headerlink" title="show git track files"></a>show git track files</h2><p><code>git ls-files</code> </p><h2 id="git-diff-one-file-in-two-different-commit-ids"><a href="#git-diff-one-file-in-two-different-commit-ids" class="headerlink" title="git diff one file in two different commit ids:"></a>git diff one file in two different commit ids:</h2><p><code>git diff HEAD(^^)(two commits before current) commit_id XX.java</code><br><code>git diff HEAD^^ HEAD main.c</code></p><h2 id="Git-RSA-key-fingerprint"><a href="#Git-RSA-key-fingerprint" class="headerlink" title="Git RSA key fingerprint"></a>Git RSA key fingerprint</h2><p>The newer SSH commands will list fingerprints as a SHA256 Key.</p><p>For example</p><p><code>ssh-keygen -lf ~/.ssh/id_rsa.pub</code><br><code>1024 SHA256:19n6fkdz0qqmowiBy6XEaA87EuG/jgWUr44ZSBhJl6Y (DSA)</code></p><p>If you need to compare it against a old fingerprint you also need to specify to use the md5 fingerprint hashing function.</p><p><code>ssh-keygen -E md5 -lf ~/.ssh/id_rsa.pub</code><br><code>2048 MD5:4d:5b:97:19:8c:fe:06:f0:29:e7:f5:96:77:cb:3c:71 (DSA)</code></p><h2 id="Git-submodule"><a href="#Git-submodule" class="headerlink" title="Git submodule:"></a>Git submodule:</h2><h3 id="Git-submodule-delete"><a href="#Git-submodule-delete" class="headerlink" title="Git submodule delete:"></a>Git submodule delete:</h3><ol><li>Delete the relevant section from the .gitmodules file.</li><li>Stage the .gitmodules changes git add .gitmodules</li><li>Delete the relevant section from .git/config.</li><li>Run git rm –cached path_to_submodule (no trailing slash).</li><li>Run rm -rf .git/modules/path_to_submodule</li><li>Commit git commit -m “Removed submodule <name>“</name></li><li>Delete the now untracked submodule files</li><li>rm -rf path_to_submodule</li></ol>]]></content>
    
    <summary type="html">
    
      Git summary。
    
    </summary>
    
      <category term="summary" scheme="http://frankchu0229.github.io/categories/summary/"/>
    
    
      <category term="notes" scheme="http://frankchu0229.github.io/tags/notes/"/>
    
      <category term="linux" scheme="http://frankchu0229.github.io/tags/linux/"/>
    
      <category term="git" scheme="http://frankchu0229.github.io/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu 命令行快捷键</title>
    <link href="http://frankchu0229.github.io/2016/12/21/linuxCmdNotes/"/>
    <id>http://frankchu0229.github.io/2016/12/21/linuxCmdNotes/</id>
    <published>2016-12-21T09:00:02.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="常用快捷键"><a href="#常用快捷键" class="headerlink" title="常用快捷键"></a>常用快捷键</h1><ul><li><code>Ctrl + a</code> 光标移动到行首</li><li><code>Ctrl + e</code> 光标移动到行尾</li><li><code>Ctrl + l</code> 清屏</li><li><code>Ctrl + Shift + c</code> 复制</li><li><code>Ctrl + Shift + v</code> 粘贴</li><li><code>Ctrl + r</code> 逆向搜索命令历史</li><li><code>Ctrl + c</code> 终止命令</li><li><code>Ctrl + z</code> 挂起命令</li><li><code>Ctrl + u</code> 删除光标左边所有元素</li><li><code>Ctrl + k</code> 删除光标右边所有元素</li></ul>]]></content>
    
    <summary type="html">
    
      熟悉ubuntu命令行快捷键，可以很好的提高在ubuntu下开发的效率。
    
    </summary>
    
      <category term="ubuntu notes" scheme="http://frankchu0229.github.io/categories/ubuntu-notes/"/>
    
    
      <category term="notes" scheme="http://frankchu0229.github.io/tags/notes/"/>
    
      <category term="ubuntu" scheme="http://frankchu0229.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>Vim Notes</title>
    <link href="http://frankchu0229.github.io/2016/11/21/vim2/"/>
    <id>http://frankchu0229.github.io/2016/11/21/vim2/</id>
    <published>2016-11-21T09:00:02.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="多行编辑"><a href="#多行编辑" class="headerlink" title="多行编辑"></a>多行编辑</h1><p>用<code>vim</code>进行多行编辑是十分方便的，只需要以下步骤：</p><ol><li><code>Ctrl-v</code> 进入纵向编辑模式 </li><li>通过<code>Shift-g</code>, <code>gg</code>, <code>j</code>,<code>k</code>等选择要编辑的行</li><li>若要删除多行，则<code>d</code>即可; 若要编辑多行的内容，<code>Shift-i</code>进入INSERT模式</li><li>进行编辑，然后按两次<code>ESC</code>就会发现多行编辑完成。</li></ol><h1 id="分屏"><a href="#分屏" class="headerlink" title="分屏"></a>分屏</h1><p>在使用<code>vim</code>编辑器的时候，结合<code>vim</code>的分屏功能，可以使得编辑更加高效：</p><h2 id="进入分屏的几种方式："><a href="#进入分屏的几种方式：" class="headerlink" title="进入分屏的几种方式："></a>进入分屏的几种方式：</h2><ol><li><code>vim -On file1 file2</code> 将<code>file1</code>和<code>file2</code>进行垂直分屏</li><li><code>vim -on file1 file2</code> 将<code>file1</code> 和<code>file2</code>进行水平分屏</li><li><code>vim file</code> 然后通过：<ul><li><code>Ctrl-w s</code>对当前文件进行水平(上下)分屏</li><li><code>Ctrl-w v</code>对当前文件进行垂直(左右)分屏</li><li><code>:sp newFile</code> 打开新的文件，上下分割 </li><li><code>:vsp newFile</code> 打开新的文件，左右分割</li></ul></li></ol><h2 id="分屏的切换："><a href="#分屏的切换：" class="headerlink" title="分屏的切换："></a>分屏的切换：</h2><ol><li><code>Ctrl-w j</code> 切换到下边的分屏上</li><li><code>Ctrl-w k</code> 切换到上边的分屏上</li><li><code>Ctrl-w h</code> 切换到左边的分屏上</li><li><code>Ctrl-w l</code> 切换到右边的分屏上</li></ol><h2 id="分屏的关闭："><a href="#分屏的关闭：" class="headerlink" title="分屏的关闭："></a>分屏的关闭：</h2><ol><li><code>Ctrl-w c</code> 关闭当前分屏</li><li><code>Ctrl-w q</code> 关闭当前分屏，若为最后一个分屏，则退出<code>vim</code>.</li></ol><p>用分屏是很方便的，当用<code>:sp newFile</code>打开一个新的文件时，可通过<code>y</code>, <code>p</code>的方式进行复制、粘贴，大大提高效率。</p><h2 id="vim-delete-part-of-lines"><a href="#vim-delete-part-of-lines" class="headerlink" title="vim delete part of lines"></a>vim delete part of lines</h2><p><code>:%s/&quot;: {//</code></p><h2 id="delete-the-whole-line-satisfied-with-the-pattern"><a href="#delete-the-whole-line-satisfied-with-the-pattern" class="headerlink" title="delete the whole line satisfied with the pattern:"></a>delete the whole line satisfied with the pattern:</h2><p><code>:g/^9 /d</code></p>]]></content>
    
    <summary type="html">
    
      vim 进阶
    
    </summary>
    
      <category term="vim notes" scheme="http://frankchu0229.github.io/categories/vim-notes/"/>
    
    
      <category term="notes" scheme="http://frankchu0229.github.io/tags/notes/"/>
    
      <category term="vim" scheme="http://frankchu0229.github.io/tags/vim/"/>
    
  </entry>
  
  <entry>
    <title>Vim Notes</title>
    <link href="http://frankchu0229.github.io/2016/10/21/vimNotes/"/>
    <id>http://frankchu0229.github.io/2016/10/21/vimNotes/</id>
    <published>2016-10-21T09:00:02.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Lesson-1-SUMMARY"><a href="#Lesson-1-SUMMARY" class="headerlink" title="Lesson 1 SUMMARY:"></a>Lesson 1 SUMMARY:</h2><ol><li><p>The cursor is moved using either the arrow keys or the hjkl keys: </p><p>h(left) , j (down) , k (up) , l (right)</p></li><li><p>To start Vim from the shell prompt type:  <code>vim FILENAME &lt;ENTER&gt;</code></p></li><li><p>To exit Vim type:     <code>&lt;ESC&gt;   :q!     &lt;ENTER&gt;</code> to trash all changes.</p><p>OR type: <code>&lt;ESC&gt;  :wq     &lt;ENTER&gt;</code> to save the changes.</p></li><li><p>To delete the character at the cursor type:  <code>x</code></p></li><li><p>To insert or append text type:</p><p> <code>i</code>   type inserted text   <code>&lt;ESC&gt;</code>   insert before the cursor</p><p> <code>A</code>   type appended text   <code>&lt;ESC&gt;</code>    append after the line</p></li></ol><p><strong>NOTE:</strong> Pressing <esc> will place you in Normal mode or will cancel an unwanted and partially completed command.</esc></p><h2 id="Lesson-2-SUMMARY"><a href="#Lesson-2-SUMMARY" class="headerlink" title="Lesson 2 SUMMARY:"></a>Lesson 2 SUMMARY:</h2><ol><li><p>To delete from the cursor up to the next word type: <code>dw</code></p></li><li><p>To delete from the cursor to the end of a line type: <code>d$</code></p></li><li><p>To delete a whole line type:  <code>dd</code></p></li><li><p>To repeat a motion prepend it with a number: <code>2w</code></p></li><li><p>The format for a change command is: <code>operator   [number]   motion</code>, where: </p><p>1). operator: is what to do, such as  <code>d</code>  for delete</p><p>2). [number]: is an optional count to repeat the motion</p><p>3). motion : moves over the text to operate on, such as  <code>w</code> (word), $ (to the end of line), etc.</p></li><li><p>To move to the start of the line use a zero:  <code>0</code></p></li><li><p>To undo previous actions, type: <code>u</code>  (lowercase u)</p><p>To undo all the changes on a line, type:  <code>U</code>  (capital U)</p><p>To undo the undo’s, type: <code>CTRL-R</code></p></li></ol><h2 id="Lesson-3-SUMMARY"><a href="#Lesson-3-SUMMARY" class="headerlink" title="Lesson 3 SUMMARY:"></a>Lesson 3 SUMMARY:</h2><ol><li><p>To put back text that has just been deleted, type <code>p</code> .  This puts the deleted text AFTER the cursor (if a line was deleted it will go on the line below the cursor).</p></li><li><p>To replace the character under the cursor, type <code>r</code> and then the character you want to have there.</p></li><li><p>The change operator allows you to change from the cursor to where the motion takes you.  e.g., type  ce  to change from the cursor to the end of the word,  <code>c$</code> to change to the end of a line.</p></li><li><p>The format for change is:  <code>c   [number]   motion</code></p></li></ol><h2 id="Lesson-4-SUMMARY"><a href="#Lesson-4-SUMMARY" class="headerlink" title="Lesson 4 SUMMARY:"></a>Lesson 4 SUMMARY:</h2><ol><li><p><code>CTRL-G</code>  displays your location in the file and the file status.</p><p><code>G</code>  moves to the end of the file.</p><p><code>number  G</code>  moves to that line number.</p><p><code>gg</code>  moves to the first line.</p></li><li><p>Typing  <code>/</code>    followed by a phrase searches FORWARD for the phrase.</p><p>Typing  <code>?</code>    followed by a phrase searches BACKWARD for the phrase.</p><p>After a search type  <code>n</code>  to find the next occurrence in the same direction or  <code>N</code>  to search in the opposite direction.</p><p><code>CTRL-O</code> takes you back to older positions, <code>CTRL-I</code> to newer positions.</p></li><li><p>Typing  <code>%</code>    while the cursor is on a ( , ),[ , ],{ , or } goes to its match.</p></li><li><p>To substitute new for the first old in a line type    <code>:s/old/new</code></p><p>To substitute new for all ‘old’s on a line type     <code>:s/old/new/g</code></p><p>To substitute phrases between two line #’s type     <code>:#,#s/old/new/g</code></p><p>To substitute all occurrences in the file type      <code>:%s/old/new/g</code></p><p>To ask for confirmation each time add ‘c’           <code>:%s/old/new/gc</code></p></li></ol><h2 id="Lesson-5-SUMMARY"><a href="#Lesson-5-SUMMARY" class="headerlink" title="Lesson 5 SUMMARY:"></a>Lesson 5 SUMMARY:</h2><p> <code>:!command</code>  executes an external command,  and some useful examples are:</p><table><thead><tr><th>MS-DOS</th><th style="text-align:center">Unix</th><th style="text-align:right">effect</th></tr></thead><tbody><tr><td><code>:!dir</code></td><td style="text-align:center"><code>:!ls</code></td><td style="text-align:right">shows a directory listing.</td></tr><tr><td><code>:!del FILENAME</code></td><td style="text-align:center"><code>:!rm FILENAME</code></td><td style="text-align:right">removes file FILENAME.</td></tr></tbody></table><ol><li><p><code>:w FILENAME</code>  writes the current Vim file to disk with name FILENAME.</p></li><li><p><code>v  motion  :w FILENAME</code>  saves the Visually selected lines in file FILENAME.</p></li><li><p><code>:r FILENAME</code>  retrieves disk file FILENAME and puts it below the cursor position.</p></li><li><p><code>:r !dir</code>  reads the output of the dir command and puts it below the cursor position.</p></li></ol><h2 id="Lesson-6-SUMMARY："><a href="#Lesson-6-SUMMARY：" class="headerlink" title="Lesson 6 SUMMARY："></a>Lesson 6 SUMMARY：</h2><ol><li><p>Type  <code>o</code>  to open a line BELOW the cursor and start Insert mode.</p><p>Type  <code>O</code>  to open a line ABOVE the cursor.</p></li><li><p>Type  <code>a</code>  to insert text AFTER the cursor.</p><p>Type  <code>A</code>  to insert text after the end of the line.</p></li><li><p>The  <code>e</code>  command moves to the end of a word.</p></li><li><p>The  <code>y</code>  operator yanks (copies) text,  <code>p</code>  puts (pastes) it.</p></li><li><p>Typing a capital <code>R</code>  enters Replace mode until  <code>&lt;ESC&gt;</code>  is pressed.</p></li><li><p>Typing “:set xxx” sets the option “xxx”.  Some options are:</p><p>1) ‘ic’ ‘ignorecase’    ignore upper/lower case when searching</p><p>2) ‘is’ ‘incsearch’    show partial matches for a search phrase</p><p>3) ‘hls’ ‘hlsearch’    highlight all matching phrases</p><p>4) You can either use the long or the short option name.</p></li></ol><ol><li>Prepend “no” to switch an option off:   <code>:set noic</code></li></ol><h2 id="Lesson-7-SUMMARY："><a href="#Lesson-7-SUMMARY：" class="headerlink" title="Lesson 7 SUMMARY："></a>Lesson 7 SUMMARY：</h2><ol><li><p>Type  <code>:help</code>  or press <code>&lt;F1&gt;</code> or <code>&lt;Help&gt;</code>  to open a help window.</p></li><li><p>Type  <code>:help</code> cmd  to find help on  cmd .</p></li><li><p>Type  <code>CTRL-W CTRL-W</code>  to jump to another window</p></li><li><p>Type  <code>:q</code>  to close the help window</p></li><li><p>Create a <strong>vimrc</strong> startup script to keep your preferred settings.</p></li><li><p>When typing a  <code>:</code>  command, press <code>CTRL-D</code> to see possible completions. Press <code>&lt;TAB&gt;</code> to use one completion.</p></li></ol>]]></content>
    
    <summary type="html">
    
      This is the summary from vimtutor, you can find all the basic vim usages here.
    
    </summary>
    
      <category term="vim notes" scheme="http://frankchu0229.github.io/categories/vim-notes/"/>
    
    
      <category term="notes" scheme="http://frankchu0229.github.io/tags/notes/"/>
    
      <category term="vim" scheme="http://frankchu0229.github.io/tags/vim/"/>
    
  </entry>
  
  <entry>
    <title>Recommended Books</title>
    <link href="http://frankchu0229.github.io/2016/09/15/books/"/>
    <id>http://frankchu0229.github.io/2016/09/15/books/</id>
    <published>2016-09-15T14:56:05.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><ul><li><a href="http://union.click.jd.com/jdc?d=sp9Y9L" target="_blank" rel="external">机器学习</a> by 周志华: 周志华老师的这本书非常适合作为机器学习入门的书籍，书中的例子十分形象且简单易懂。</li><li><a href="http://s.click.taobao.com/t?e=m%3D2%26s%3Dvoz%2B2wUYDVIcQipKwQzePOeEDrYVVa64K7Vc7tFgwiHjf2vlNIV67im%2FDi4clQ3Zz%2BnB8CFd%2BwI%2FLTvqqHUmN%2FSQc5E%2BACfRlCN%2FCfnE38N8UtmKu0ijLvTWyv95D3bGhJCy0erBlaJPav5W3RZfiE7ABVnJDlblxg5p7bh%2BFbQ%3D&amp;pvid=10_124.76.32.119_23247_1476627551167" target="_blank" rel="external">统计机器学习</a> by 李航：李航老师的这本书偏优化和推倒，推倒相应算法的时候可以参考这本书。</li><li><a href="http://www.rmki.kfki.hu/~banmi/elte/Bishop%20-%20Pattern%20Recognition%20and%20Machine%20Learning.pdf" target="_blank" rel="external">PRML</a> by Christopher Bishop: PRML这本书有点偏Bayesian了，初学者看起来可能有些困难，可以和前两本结合起来看。</li><li><a href="http://www.vision.jhu.edu/gpca/" target="_blank" rel="external">GPCA</a> by Yi Ma: 这本书由马毅老师耗时十年精心打造，推荐阅读。</li><li><a href="https://mitpress.mit.edu/books/machine-learning-0" target="_blank" rel="external">Machine Learning A Probabilistic Perspective</a> by Kevin P. Murphy: MLAPP这本书也是一本比较经典的机器学习书，可以和PRML互相补充着来看。</li></ul><h2 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h2><ul><li><a href="http://union.click.jd.com/jdc?d=EfwjWU" target="_blank" rel="external">数学之美</a> by 吴军：吴军老师的这本书适合作为入门自然语言处理的科普读物。</li><li><a href="http://union.click.jd.com/jdc?d=PjQvgb" target="_blank" rel="external">统计自然语言处理</a> by 宗成庆：中文版的自然语言处理图书是比较少的，这本书由中科院宗成庆老师所写，推荐初学者先阅读此书。</li><li><a href="http://nlp.stanford.edu/fsnlp/" target="_blank" rel="external">Foundations of Statistical Natural Language Processing</a> by Christopher D. Manning: 本书由Manning大神所写，在1999年出版，最近比较火的Deep Learning for NLP没有涉及，不过可以参考他的学生Socher开的这门课 <a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="external">CS224n - Natural Language Processing with Deep Learning</a>.</li><li><a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="external">Speech and Language Processing</a> by Dan Jurafsky: 这本书第三版已经更新一部分章节了，书中介绍了deep learning for nlp方面的技术，推荐阅读。</li></ul><h2 id="To-be-continued"><a href="#To-be-continued" class="headerlink" title="To be continued."></a>To be continued.</h2>]]></content>
    
    <summary type="html">
    
      本文介绍了AI方面常用的书籍。
    
    </summary>
    
      <category term="books" scheme="http://frankchu0229.github.io/categories/books/"/>
    
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/tags/Machine-Learning/"/>
    
      <category term="Natural Language Processing" scheme="http://frankchu0229.github.io/tags/Natural-Language-Processing/"/>
    
      <category term="Artificial Intelligence" scheme="http://frankchu0229.github.io/tags/Artificial-Intelligence/"/>
    
      <category term="Data Mining" scheme="http://frankchu0229.github.io/tags/Data-Mining/"/>
    
      <category term="Java" scheme="http://frankchu0229.github.io/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Ensemble Learning</title>
    <link href="http://frankchu0229.github.io/2016/06/15/ML9/"/>
    <id>http://frankchu0229.github.io/2016/06/15/ML9/</id>
    <published>2016-06-15T14:56:05.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>集成学习 (ensemble learning) 通过组合多个学习器来完成学习任务。相比于单个学习器，通过集成学习得到的模型往往可以取得更好的performance。因此在天池、kaggle和ImageNet等各大竞赛中，最终的模型往往都是通过集成学习得到的。</p><p>集成学习并不能保证集成后的模型一定比单个的模型效果好。如果各个学习器比较接近，那么集成后的模型不会有太多performance上的提升；如果各个学习器完全相同，那么集成后的模型不会有性能的提升。所以要想获得好的集成效果，需要各个学习器准确性不能太差，同时要有多样性，即“好而不同”。然而准确性和多样性本身就存在冲突。一般来说，准确性很高之后，要增加多样性就要牺牲准确性。所以如何产生并结合“好而不同”的学习器，也是集成学习研究的核心内容。</p><p>那么怎样将各个学习器组合起来呢？对于分类任务来讲，最简单的方法就是投票；对于回归任务来讲，可以通过加权平均的方式来组合各个学习器。那么有没有更好的做法呢？Boosting, bagging 和 random forest是常用的集成学习算法，看完下文对它们的介绍，相信你会有了答案。</p><h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>Boosting是一种可将弱学习器提升为强学习器的算法。通常boosting的做法是：首先在原始数据上训练出一个基学习器；用这个基学习器来改变数据的分布，即增大在训练中出错样本的权重，使得出错的样本在后续的训练中受到更大的关注；减小未出错样本的权重；然后在调整分布后的样本训练集上训练新的基学习器；如此重复进行，直到基学习器的数量达到了预先设定的个数\(T\)，最终将这\(T\)个基学习器进行加权组合。</p><p>Boosting中最著名的代表就是adaboost (adaptive boosting)算法了：</p><h3 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h3><p>输入：数据集 \(D = \{(x_i, y_1), (x_2, y_2), …, (x_n, y_n) \} \), 基学习算法 \(L\)，训练轮数\(T\).</p><p>输出：集成后的学习器 \(H(x)\)</p><p>步骤：</p><ol><li>初始化训练数据权值分布：\(W_1 = \{ w_{11}, w_{12}, …, w_{1n} \}\), \(w_{1i} = \frac{1}{n}\), i = 1,2,3…n</li><li>for \(t = 1 : T\) do:</li><li>在分布为\(W_t\)的训练集上训练基学习器 \(h_t = L(D, W_t) \)</li><li>计算基学习器 \(h_t\)在训练数据上的错误率：\(e_t = \sum_{i = 1}^{n} w_{ti}  \ I(h_t(x_i) \neq y_i)\), 其中\(I\) 为 indicator function。</li><li>计算基学习器加权系数：\(\alpha_{t} = \frac{1}{2} \ \ln\frac{1 - e_t}{e_t}\)。 当\(e_t \leq \frac{1}{2} \)时，\(e_t\)越小， 加权系数\(\alpha_t \)越大，在最终的模型中起的作用就越大。</li><li>if \(e_t &gt; 0.5\), then break;</li><li>更新训练数据权值分布： \(W_{t+1} = \frac{W_t \exp{(-\alpha_t f(x) h_t(x))}}{Z_t}\), 其中\(Z_t\)为normalization factor。 展开来看有：若训练正确，则权重减小： \(w_{t+1, i} = \frac{w_{t,i} \exp{(-\alpha_t)}}{Z_t}\)； 若训练错误，权重增大： \(w_{t+1, i} = \frac{w_{t,i} \exp{(\alpha_t)}}{Z_t}\)</li><li>end for</li><li>\(H(x) = sign(\sum_{t = 1}^{T} \alpha_t h_t(x))\), 其中\(sign(x)\) 为符号函数。</li></ol><h4 id="Adaboost-算法解释"><a href="#Adaboost-算法解释" class="headerlink" title="Adaboost 算法解释"></a>Adaboost 算法解释</h4><p>Adaboost算法还有另一种解释，即认为损失函数为指数函数\(E = \sum_{i = 1}^{n} \exp{(-\frac{1}{2} y_i \ F(x_i))}\), 其中 \(F(x) = \sum_{t = 1}^{T} \alpha_t h_t(x)\)， 需要求的参数是\(\alpha_t\)和各个基学习器中的参数。对于第\(m\)个基学习器来说，我们可以认为前\(m-1\)个基学习器是fixed的，所以可以通过优化损失函数来求得\(\alpha_m\)和\(h_m(x)\)中的参数。具体细节可以查看<a href="prml">PRML</a>中的659-663页。</p><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><h2 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h2><h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><h2 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Book: <a href="prml">Pattern Recognition and Machine Learning by Christopher Bishop</a></li><li><a href="http://www.holehouse.org/mlclass" target="_blank" rel="external">Notes of machine learning by Andrew Ng in cousera</a></li><li><a href="http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm" target="_blank" rel="external">机器学习</a> 周志华</li><li>Slides by <a href="http://sist.shanghaitech.edu.cn/StaffDetail.asp?id=334" target="_blank" rel="external">Prof. Wang</a></li><li><a href="http://www.hangli-hl.com/books.html" target="_blank" rel="external">统计学习方法</a></li><li><a href="(http://yima.csl.illinois.edu/">GPCA by Yi Ma</a>)</li></ul><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>]]></content>
    
    <summary type="html">
    
      Ensemble learning 常用来将若干个训练好的模型结合起来，以得到更好的performance。本文介绍了常用的ensemble learning算法： boosting， bagging 和 random forest，也介绍了常用在ensemble learning中的KNN 和 decision tree算法。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/tags/Machine-Learning/"/>
    
      <category term="Ensemble Learning" scheme="http://frankchu0229.github.io/tags/Ensemble-Learning/"/>
    
      <category term="Boosting" scheme="http://frankchu0229.github.io/tags/Boosting/"/>
    
      <category term="Bagging" scheme="http://frankchu0229.github.io/tags/Bagging/"/>
    
      <category term="Random Forest" scheme="http://frankchu0229.github.io/tags/Random-Forest/"/>
    
      <category term="KNN" scheme="http://frankchu0229.github.io/tags/KNN/"/>
    
      <category term="Decision Tree" scheme="http://frankchu0229.github.io/tags/Decision-Tree/"/>
    
  </entry>
  
  <entry>
    <title>Clustering</title>
    <link href="http://frankchu0229.github.io/2016/05/15/ML8/"/>
    <id>http://frankchu0229.github.io/2016/05/15/ML8/</id>
    <published>2016-05-15T14:56:05.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h1><p>聚类 (clustering)是机器学习中经常会涉及到的一个task。通常来讲，它是一种unsupervised task，通过挖掘数据内部的结构等信息，来将数据划分成若干部分，也有利用数据的label信息来做辅助的方法，如Learning Vector Quantization (LVQ)。</p><h2 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h2><p>提到聚类，最常用的方法就是K-means了。K-means的思想比较简单：对于一组数据，我们希望找到一组中心点，使得整个数据集中所有点到离它最近的中心点的距离之和最小，相应的数学表达式为：</p><p>$$ E = \sum_{i=1}^{k}\sum_{x \in C_i} ||x - \mu_i||_2^2$$</p><p>其中，\(k\)为簇的总数，\(C_i\)为相应的第\(i\)个簇。若直接对\(E\)进行求解很困难(NP-hard)，因为有指数多个可能的簇划分。所以这里采用迭代优化的方法来近似求解。</p><ol><li>K-means首先初始化k个中心点，这里需要提到的是k-means的初始化对最终结果有着很大的影响，不同的初始化会有着不同的聚类效果。</li><li>对数据集中的所有点，找到距离它最近的中心点，这样就得到了新的簇的划分。相应的数学表达式为：\(i = \arg\min_{i=1,2,…,k} ||x - \mu_i||_2^2\)</li><li>在新形成的k个簇中，更新k个相应的中心点。相应的数学表达式为：\(\mu_i = \frac{\sum_{x \in C_i} \ x} {|C_i|}\)。</li><li>重复步骤2和步骤3直到达到迭代次数或者收敛 (E的值变化很小)。</li></ol><h3 id="k-means中存在的两个问题"><a href="#k-means中存在的两个问题" class="headerlink" title="k-means中存在的两个问题"></a>k-means中存在的两个问题</h3><p>k-means中有两个问题需要值得注意：</p><ol><li>k-means中的中心点怎么初始化 </li><li>k-means中的k怎么选取</li></ol><h4 id="k-means中心点初始化"><a href="#k-means中心点初始化" class="headerlink" title="k-means中心点初始化"></a>k-means中心点初始化</h4><p>对于中心点的初始化，我们希望初始化的中心点能在数据点附近，同时中心点尽量分散开。 如果中心点初始化的比较近的话，很有可能就都初始化在同一个簇中，这样就使得原本应该划分为同一个簇的数据点被划分成了两个簇。</p><p>通常的做法有：</p><ol><li>随机初始化中心点到数据集上的k个点上。然后跑若干次，选取其中\(E\)最小的那一组初始化。这种做法的缺点在于，有可能两个中心点离的很近。</li><li>首先随机初始化一个中心点到数据集上的一个点上，第二个中心点初始化为距离第一个中心点最远的那个数据点；对于后面的中心点的初始化，我们每次初始化成所有数据点中距离前面初始化好的中心点最近距离最大的那个点。</li><li>用k-means++方式来初始化。k-means++的初始化方法与第二种方法比较像，不同之处在于k-means+＋引入了概率，即每一个数据点成为下一个中心点的概率正比于该数据点与其最近中心点距离的平方\(D(x)^2\)。</li></ol><h4 id="k-means中的k怎么选取"><a href="#k-means中的k怎么选取" class="headerlink" title="k-means中的k怎么选取"></a>k-means中的k怎么选取</h4><p>当k取值越大时，相应的E会越来越小，如果k取为数据点的总数，那么E会变成0。所以单独的E不能作为k选取的标准。一种解决方法是用\(E+complexity\)来作为衡量的标准，其中complexity与k的大小有关，k越大，相应的complexity越大。常用的衡量标准有 BIC (Bayesian Information Criterion)。还有一个在聚类中经常用到的衡量标准：CH (Calinski-Harabasz) index，有兴趣的读者可以自行google，这里不做具体介绍了。</p><h2 id="Mixtures-of-Gaussians"><a href="#Mixtures-of-Gaussians" class="headerlink" title="Mixtures of Gaussians"></a>Mixtures of Gaussians</h2><p>k-means 在某种条件下，可以看作是mixtures of Gaussians的特例。在k-means中，我们认为每一个数据点必须属于某一个簇，这个条件太硬(hard)了， 在mixtures of Gaussians中，我们认为每一个数据点按照某个概率属于某一个簇。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>数据通常满足一定的分布，对于比较简单且满足高斯分布的数据，我们可以用一个高斯分布来对这些数据来建模。然而，现实中数据的分布通常比较复杂，只用一个高斯分布建模效果不好，通常用若干个高斯分布的组合来建模数据的分布，这里我们用K个高斯分布的组合来建模数据的分布：</p><p>$$p(\mathbf{x}) = \sum_{k = 1}^{K} \pi_{k} N(\mathbf{x}|\mu_{k}, \Sigma_{k})$$ 其中\(\pi_{k}\)为高斯分布\(N(\mathbf{x}|\mu_{k}, \Sigma_{k})\)的加权系数。</p><h3 id="Hidden-Variables"><a href="#Hidden-Variables" class="headerlink" title="Hidden Variables"></a>Hidden Variables</h3><p>我们引入一个K维的hidden variable \(\mathbf{z}\)， \(\mathbf{z}\)是一个one-hot的向量，用来建模当前的变量\(\mathbf{x}\)由哪一个高斯分布生成的 (知道这个信息后，模型的表示和参数求解都会变得简单)，相应的， $$p(\mathbf{x}) = \sum_{\mathbf{z}} p(\mathbf{z}, \mathbf{x}) = \sum_{\mathbf{z}} p(\mathbf{z})  p(\mathbf{x}|\mathbf{z}) ＝\sum_{k = 1}^{K} \pi_{k} N(\mathbf{x}|\mu_{k}, \Sigma_{k})$$</p><h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h3><p>Mixtures of Gaussians是生成模型，我们用maximum likelihood estimation (MLE)来求得我们的优化目标，即：</p><p>$$ \log L(\mathbf{\pi}, \mathbf{\mu}, \mathbf{\Sigma}) = \log(\prod_{i = 1} ^ {N} \sum_{k = 1}^{K} \pi_{k} N(\mathbf{x}_{i}|\mu_{k}, \Sigma_{k})) = \sum_{i = 1}^{N} \log(\sum_{k = 1}^{K} \pi_{k} N(\mathbf{x}_{i}|\mu_{k}, \Sigma_{k})) $$</p><p>模型的参数为\(\mathbf{\pi}, \mathbf{\mu}, \mathbf{\Sigma}\), 我们通过优化目标函数来求得最优的参数值。那么如果有了学好的模型，我们怎么做prediction呢？我们可以通过计算\(p(z_k = 1 | \mathbf{x})\) 的概率，然后将点\(\mathbf{x}\)划分到概率最大的那个簇，相应的：<br>$$ p(z_k = 1 | \mathbf{x}) = \frac{p(z_k = 1) p(\mathbf{x}|z_k = 1)}{p(\mathbf{x})} = \frac{\pi_k N(\mathbf{x}|\mu_{k}, \Sigma_{k})}{\sum_{k = 1}^{K} \pi_k N(\mathbf{x}|\mu_{k}, \Sigma_{k})}$$</p><p>接下来的问题就是怎么优化目标函数来求得最优参数的值了，对于含有隐变量的优化问题，我们通常用Expectation Maximization (EM)来求解。</p><h2 id="Expectation-Maximization-EM"><a href="#Expectation-Maximization-EM" class="headerlink" title="Expectation Maximization (EM)"></a>Expectation Maximization (EM)</h2><p>EM算法是一种常用的优化算法，多用在含有隐变量的优化问题上。通过MLE，我们可以得到相应的优化目标，即：</p><p>$$ \log L(\mathbf{\theta}) = \log \prod_{i = 1}^{N} p(\mathbf{x}) = \sum_{i = 1}^{N} \log p(\mathbf{x}) = \sum_{i = 1}^{N} \log \sum_{\mathbf{z}}p(\mathbf{x}, \mathbf{z}) = \sum_{i = 1}^{N} \log \sum_{\mathbf{z}}p(\mathbf{x}, \mathbf{z}) $$</p><p>由于对数函数为凹函数，则由Jesen不等式，我们可以得到：</p><p>$$\log L(\mathbf{\theta}) = \sum_{i = 1}^{N} \log \sum_{\mathbf{z}_i}p(\mathbf{x}_i, \mathbf{z}_i) = \sum_{i = 1}^{N} \log \sum_{\mathbf{z}_i}Q(\mathbf{z}_i) \frac{p(\mathbf{x}_i, \mathbf{z}_i)}{Q(\mathbf{z}_i)} \geq   \sum_{i = 1}^{N} \sum_{\mathbf{z}_i}Q(\mathbf{z}_i) \log \frac{p(\mathbf{x}_i, \mathbf{z}_i)}{Q(\mathbf{z}_i)} $$</p><p>其中，\(Q(\mathbf{z}_i)\) 为关于隐变量\(\mathbf{z}_i\)的一个分布。经过放缩后，对于任意一个\(Q(\mathbf{z}_i)\)，我们都可以得到目标函数的一个下界，那么怎么选取\(Q(\mathbf{z}_i)\)呢？对于特定的参数\(\mathbf{\theta}\), 我们可以选取\(Q(\mathbf{z}_i)\), 使得上述不等式关系中的相等关系成立。由Jesen不等式可知，当且仅当\(E(f(x)) = f(E(x))\), 等式成立，即 \(\frac{p(\mathbf{x}_i, \mathbf{z}_i)}{Q(\mathbf{z}_i)} = c\), 相应的，\(Q(\mathbf{z}_i) \propto p(\mathbf{x}_i, \mathbf{z}_i)\). 又由\(\sum_{\mathbf{z}_i} Q(\mathbf{z}_i) = 1\), 得到\( Q(\mathbf{z}_i) = p(\mathbf{z}_i | \mathbf{x}_i, \theta)\).</p><p>由此得到EM算法: 首先我们初始化参数\(\theta\), 然后经过E-step 和 M-step若干次迭代，直到收敛。</p><ul><li>E-step: For each i, set</li></ul><p>$$ Q(\mathbf{z}_i) = p(\mathbf{z}_i | \mathbf{x}_i, \theta) $$</p><ul><li>M-step, set:</li></ul><p>$$ \theta = \arg \max_{\theta}  \sum_{i = 1}^{N} \sum_{\mathbf{z}_i}Q(\mathbf{z}_i) \log \frac{p(\mathbf{x}_i, \mathbf{z}_i)}{Q(\mathbf{z}_i)} $$</p><h3 id="Convergence"><a href="#Convergence" class="headerlink" title="Convergence"></a>Convergence</h3><p>EM算法一定会收敛吗？答案是肯定的。因为在E-step中，\(\log L(\theta)\)等于其下界的值；在M-step中，我们最大化下界来获得新一轮的参数\(\theta\). 然后在下一次的E-step中，\(\log L(\theta)\)又等于新的下界的值。所以在EM算法中，优化目标是不断变大的。但是EM算法只能保证局部收敛；如果优化目标是凸函数，则可以得到全局最优解。</p><h3 id="Mixture-of-Gaussians-revisited"><a href="#Mixture-of-Gaussians-revisited" class="headerlink" title="Mixture of Gaussians revisited"></a>Mixture of Gaussians revisited</h3><p>有了上面EM的推倒，相信现在可以轻易的推出Mixture of Gaussians中参数更新的公式了：</p><ul><li>E-step:</li></ul><p>$$ \gamma_{k} = p(z_k = 1 | \mathbf{x}) = \frac{\pi_k N(\mathbf{x}|\mu_{k}, \Sigma_{k})}{\sum_{k = 1}^{K} \pi_k N(\mathbf{x}|\mu_{k}, \Sigma_{k})}$$</p><ul><li>M-step:</li></ul><p>$$\mu_{k} = \frac{\sum_{i=1}^{N}\gamma_k \mathbf{x}_i}{\sum_{i=1}^{N}\gamma_k}$$</p><p>$$ \pi_k = \frac{\sum_{i = 1}^{N} \gamma_k}{N}$$</p><p>\(\Sigma_k\)可以用相似的方法来计算，这里需要注意的是在计算\( \pi_k\)最优值的时候，需要用拉格朗日来解等式约束。</p><h2 id="Spectral-Clustering-amp-amp-N-cut"><a href="#Spectral-Clustering-amp-amp-N-cut" class="headerlink" title="Spectral Clustering &amp;&amp; N-cut"></a>Spectral Clustering &amp;&amp; N-cut</h2><p>In most cases, the distribution of a mixed dataset can be more complicated than simply clustering around a few cluster centers. In this case, the K-means and mixtures of Gaussians may not group the data correctly. The following figure gives an example where the K-means and mixtures of Gaussians may not group the data correctly. </p><div align="center"><br><img src="/img/spectralClustering.jpg" width="500" height="300" align="center"><br></div><p>一种解决上述问题的方法是将数据点经过某个合适的非线性变换 (e.g., Laplacian Eigenmaps) ，转换到合适的形式 (例如上面图中的例子)。 Spectral clustering和 Normalized cut (N-cut)算法就是应用了这种思想。关于这两个算法的细节这里不再详细列出，有兴趣的读者可以去看马毅老师的新书<a href="(http://yima.csl.illinois.edu/">GPCA by Yi Ma</a>)。</p><h2 id="Hierarchical-Clustering-层次聚类"><a href="#Hierarchical-Clustering-层次聚类" class="headerlink" title="Hierarchical Clustering (层次聚类)"></a>Hierarchical Clustering (层次聚类)</h2><p>层次聚类试图在不同层次上来对数据进行划分，进而形成树形的聚类结构。它对数据的划分是一个“自底向上”的过程。该算法的思想也比较简单：</p><ol><li>数据集中的每一个样本为一个初始簇</li><li>将距离最近的两个簇合并</li><li>不断重复步骤2，直到簇的个数达到指定的个数为止。</li></ol><p>距离的衡量方式有以下几种：</p><ol><li>两个簇中所有点间距离的最小值</li><li>两个簇中所有点间距离的最大值</li><li>两个簇的中心点之间的距离</li><li>两个簇中所有点间距离的平均值</li></ol><h2 id="Other-Methods"><a href="#Other-Methods" class="headerlink" title="Other Methods"></a>Other Methods</h2><h3 id="Density-based-Clustering"><a href="#Density-based-Clustering" class="headerlink" title="Density-based Clustering"></a>Density-based Clustering</h3><p>密度聚类是通过样本数据分布的紧密程度来划分数据的，在这里我们主要介绍DBSCAN算法：</p><ol><li>DBSCAN算法先找出数据集中各样本的\(\epsilon\)-邻域, 并确定核心对象集合\(\Omega\)</li><li>从核心对象集合\(\Omega\)中选取一个核心对象作为种子，找出由它密度可达的所有样本，这就构成了一个聚类簇。</li><li>然后从\(\Omega\)集合中去除步骤2中的核心对象</li><li>重复步骤2和3直到\(\Omega\)为空。</li></ol><p>这个算法中涉及到的相关概念可以参考周志华老师<a href="http://union.click.jd.com/jdc?d=sp9Y9L" target="_blank" rel="external">机器学习</a>这本书。</p><h3 id="Learning-Vector-Quantization-LVQ"><a href="#Learning-Vector-Quantization-LVQ" class="headerlink" title="Learning Vector Quantization (LVQ)"></a>Learning Vector Quantization (LVQ)</h3><p>LVQ和其他聚类方法不同，LVQ假设数据有类别标记，并利用样本的类别标记信息来辅助聚类：</p><ol><li>LVQ的目标是学得一组n维原型向量\(\{p_1, p_2,…p_q \} \), 每个原型向量用来代表一个聚类簇，簇标记\(t_i \in \Lambda\)</li><li>初始化原型变量，并为各个原型变量预设标记\(t_i\)</li><li>从数据集中随机选取一样本，并找到和该点距离最近的原型向量。</li><li>如果该样本点和原型向量的类别标记相同，则使该原型向量靠近该样本点一些，否则远离该样本点一些。</li><li>重复步骤3和4直到收敛。</li></ol><p>有了学好的一组原型向量\(\{p_1, p_2,…p_q \} \)，在做inference的时候，我们通过找距离test样本距离最近的原型向量，来得到该test样本点所属的簇。LVQ通常用来改变类别标记的颗粒度大小，例如，如果想使类别标记的更细一些，则可以使原型向量的个数大于原来类别标记种类的数量。</p><p>更多细节可以参考周志华老师新书：<a href="http://union.click.jd.com/jdc?d=sp9Y9L" target="_blank" rel="external">机器学习</a>。</p><h2 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h2><p>聚类有很多的应用场景，例如在图像压缩中，通过存储图像聚类后的中心点来进行图像压缩；聚类也常用来进行异常检测 (离群点检测)，可将远离所有簇中心的样本作为异常点，或将密度极低处的样本作为异常点。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>Book: <a href="prml">Pattern Recognition and Machine Learning by Christopher Bishop</a></li><li><a href="http://www.holehouse.org/mlclass" target="_blank" rel="external">Notes of machine learning by Andrew Ng in cousera</a></li><li><a href="http://union.click.jd.com/jdc?d=sp9Y9L" target="_blank" rel="external">机器学习</a> 周志华</li><li>Slides by <a href="http://sist.shanghaitech.edu.cn/StaffDetail.asp?id=334" target="_blank" rel="external">Prof. Wang</a></li><li><a href="http://www.hangli-hl.com/books.html" target="_blank" rel="external">统计学习方法</a></li><li><a href="(http://yima.csl.illinois.edu/">GPCA by Yi Ma</a>)</li></ul><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>]]></content>
    
    <summary type="html">
    
      聚类 (clustering)是机器学习中经常会涉及到的一个task, 本篇文章从k-means开始讲起，然后到Mixtures of Gaussians和EM算法，最后介绍层次聚类、密度聚类和Learning Vector Quantization (LVQ)。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/tags/Machine-Learning/"/>
    
      <category term="Clustering" scheme="http://frankchu0229.github.io/tags/Clustering/"/>
    
  </entry>
  
  <entry>
    <title>Support Vector Machines</title>
    <link href="http://frankchu0229.github.io/2016/05/09/ML7/"/>
    <id>http://frankchu0229.github.io/2016/05/09/ML7/</id>
    <published>2016-05-09T14:56:05.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Support-Vector-Machines"><a href="#Support-Vector-Machines" class="headerlink" title="Support Vector Machines"></a>Support Vector Machines</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在logistic regression中，分类器只要将两个类别的点分开即可使cost function达到最小，但是哪一种分类器更好呢？下面的三个logistic regression分类器都能将两个类别的点完全分开。从图中我们可以直观的看出，距离分离平面越近的点越容易分类错误，而距离分离平面越远的点则越不容易分类错误。下面三幅图中，图三中的点最不容易分类错误。所以我们想使所有的点分类正确，同时所有的点距离分离平面尽可能的远，可以理解为我们要最大化所有点中到分类平面的最小距离, 这也是SVM的目标。</p><div align="center"><br><img src="/img/logisticRegression.png" width="500" height="200" align="center"><br></div><p>我们知道点\(x_i\)到分类平面的几何距离为\(\frac{|w^Tx_i+b|}{||w||}\),而如果该平面能够作为分类平面，则必有：\(y_i(w^Tx_i+b) &gt; 0\). 我们的目标是最大化所有点到分离平面的最小距离。通过rescaling \(w,b\),可以得到: \(\min_{i = 1,2,…m} y_i(w^Tx_i + b) = 1 \) (<strong>这个地方可能不是很好理解，需要注意的是：1.平面的w和b可以变化到相应的倍数而这个平面仍然保持不变；2.假设当前的w和b使得距离分类平面最近的点的\(y(w^Tx + b) = D\), 则我们可以将w和b缩小\(\frac{1}{D}\),从而使\(y(w^Tx + b) = 1\).</strong>),这样我们可以得到所有点中到分离平面的最小距离为: \(\frac{1}{||w||}\), 我们把这个值称为<strong>margin</strong>。由此，我们可以得到SVM的优化目标和限制条件：</p><p>$$ \max_{w,b} \ \  \frac{1}{||w||}  \\ s.t. \ \ y_i(w^Tx_i + b) \geq 1$$<br>相应地，可以变为，</p><p>$$ \min_{w,b} \ \  \frac{1}{2}||w||_{2}^{2}  \\ s.t. \ \ y_i(w^Tx_i + b) \geq 1$$</p><p>由此，SVM的目标函数就变成了一个标准的凸二次规划问题，有很多开源的优化包都可以用来解这个问题。Here is an example using the active set optimization method in matlab (first you need to transform the equations into vector form):</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">X = [ <span class="number">3</span> <span class="number">1</span>; <span class="number">3</span> <span class="number">-1</span>; <span class="number">6</span> <span class="number">1</span>; <span class="number">6</span> <span class="number">-1</span>; <span class="number">1</span> <span class="number">0</span> ; <span class="number">0</span> <span class="number">1</span>; <span class="number">0</span> <span class="number">-1</span>; <span class="number">-1</span> <span class="number">0</span>];</div><div class="line">y = [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">-1</span> <span class="number">-1</span> <span class="number">-1</span> <span class="number">-1</span>]';</div><div class="line"></div><div class="line"><span class="comment">%% plot the data:</span></div><div class="line">plot(X(<span class="number">1</span>:<span class="number">4</span>,<span class="number">1</span>),X(<span class="number">1</span>:<span class="number">4</span>,<span class="number">2</span>),<span class="string">'+'</span>)</div><div class="line">hold on</div><div class="line">plot(X(<span class="number">5</span>:<span class="number">8</span>,<span class="number">1</span>),X(<span class="number">5</span>:<span class="number">8</span>,<span class="number">2</span>),<span class="string">'o'</span>)</div><div class="line">hold on</div><div class="line"></div><div class="line"><span class="comment">%% primal problem:</span></div><div class="line">w = [<span class="number">0</span> <span class="number">0</span>]';</div><div class="line">wb = <span class="number">0</span>;</div><div class="line">theta = [wb;w];</div><div class="line">H = [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>; <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>; <span class="number">0</span> <span class="number">0</span> <span class="number">1</span>];</div><div class="line">f = [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]';</div><div class="line">b = <span class="number">-1</span>*<span class="built_in">ones</span>(<span class="built_in">size</span>(X,<span class="number">1</span>),<span class="number">1</span>);</div><div class="line">A = <span class="number">-1</span>*[y, [y,y].*X];</div><div class="line">options = optimoptions(<span class="string">'quadprog'</span>,...</div><div class="line">    <span class="string">'Algorithm'</span>,<span class="string">'active-set'</span>,<span class="string">'Display'</span>,<span class="string">'off'</span>);</div><div class="line"></div><div class="line">[theta,fval,exitflag,output,lambda] = ...</div><div class="line">   quadprog(H,f,A,b,[],[],[],[],[],options);</div><div class="line">wb = theta(<span class="number">1</span>);</div><div class="line">w = theta(<span class="number">2</span>:<span class="keyword">end</span>);</div><div class="line">X*w + wb</div></pre></td></tr></table></figure><h2 id="Dual-Problem-of-SVM"><a href="#Dual-Problem-of-SVM" class="headerlink" title="Dual Problem of SVM"></a>Dual Problem of SVM</h2><p>更常用的求解SVM的方法是求解其对偶问题。</p><p>首先我们可以将优化目标写成拉格朗日函数的形式：<br>\(L(w,b,\alpha) = \frac{1}{2}w^Tw + \sum_{i=1}^{m}\alpha_i(1-y_i(w^Tx_i+b))\), 相应的与约束条件等价的形式为: \(\max_{\alpha_i &gt; 0} L(w,b,\alpha)\)。那么关于SVM，我们可以表示为：\(\min_{w,b} \max_{\alpha_i &gt; 0} L(w,b,\alpha)\). 由于上述表达式属于强对偶的形式，所以原问题的解即为对偶问题的解。由此，我们可以得到SVM的对偶问题为：</p><p>$$ \max_{\alpha} \min_{w,b} L(w,b,\alpha) ＝  \max_{\alpha} \min_{w,b}  \frac{1}{2}w^Tw + \sum_{i=1}^{m}\alpha_i(1-y_i(w^Tx_i+b))$$</p><p>首先，求解该拉格朗日函数关于\(w,b\)的最小值的解，对\(L(w,b,\alpha)\) 对\(w,b\)求偏导可得，</p><p>$$\frac{\partial L}{\partial w} = w － \sum_{i=1}^{m} \alpha_ix_iy_i ＝ 0$$</p><p>$$\sum_{i=1}^{m}\alpha_iy_i = 0$$</p><p>将相应的\(w,b\)带入可得对偶问题的优化目标为：</p><p>$$\max_{\alpha} L(w,b,\alpha) = -\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_i^Tx_j + \sum_{i=1}^{m} \alpha_i$$</p><p>相应的对偶问题的约束条件为：</p><p>$$\alpha_i \geq 0 \\ \sum_{i=1}^{m}\alpha_iy_i = 0$$</p><p>对偶问题依然是一个凸二次规划问题，可以用常用的优化包来求解。这样在求得最优的\(\alpha^{*}\)之后，通过\(w^{*} = \sum_{i=1}^{m} \alpha^{*}_ix_iy_i\)即可求得相应的最优w值。那么相应的b怎么求呢？我们可以看原问题的KKT条件：</p><p>$$\alpha_i \geq 0 \\ 1 - y_i(w^Tx_i+b) \leq 0 \\ \alpha_i(1 - y_i(w^Tx_i+b)) = 0$$</p><p>当\(\alpha_i &gt; 0\)时，相应的有\(y_i = (w^Tx_i+b)\), 此时的(\(x_i,y_i\))即为support vectors，最优的分类平面即由这些support vectors决定。由这些support vectors，我们可以得到:</p><p>$$b^{*} = y_s - {w^{*}}^Tx_s = y_s - \sum_{i=1}^{m} \alpha^{*}_iy_ix_i^Tx_s$$</p><p>Here is an example using the optimization method in matlab to solve the dual problem of SVM (first you need to transform the equations into vector form):</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">X = [ <span class="number">3</span> <span class="number">1</span>; <span class="number">3</span> <span class="number">-1</span>; <span class="number">6</span> <span class="number">1</span>; <span class="number">6</span> <span class="number">-1</span>; <span class="number">1</span> <span class="number">0</span> ; <span class="number">0</span> <span class="number">1</span>; <span class="number">0</span> <span class="number">-1</span>; <span class="number">-1</span> <span class="number">0</span>];</div><div class="line">y = [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">-1</span> <span class="number">-1</span> <span class="number">-1</span> <span class="number">-1</span>]';</div><div class="line">a = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X,<span class="number">1</span>),<span class="number">1</span>);</div><div class="line">f = <span class="number">-1</span>*<span class="built_in">ones</span>(<span class="built_in">size</span>(X,<span class="number">1</span>),<span class="number">1</span>);</div><div class="line">lb = <span class="built_in">zeros</span>(<span class="built_in">size</span>(X,<span class="number">1</span>),<span class="number">1</span>);</div><div class="line">beq = <span class="number">0</span>;</div><div class="line">Aeq = y';</div><div class="line">G = ([y y].*X) * ([y y].*X)';</div><div class="line"></div><div class="line">options = optimoptions(<span class="string">'quadprog'</span>,...</div><div class="line">    <span class="string">'Algorithm'</span>,<span class="string">'interior-point-convex'</span>,<span class="string">'Display'</span>,<span class="string">'off'</span>);</div><div class="line"></div><div class="line">[a,fval,exitflag,output,lambda] = ...</div><div class="line">   quadprog(G,f,[],[],Aeq,beq,lb,Inf,[],options);</div><div class="line"></div><div class="line">t = a.*y;</div><div class="line">t1 = [t t].*X;</div><div class="line">w = sum(t1)'</div><div class="line">b = y(<span class="number">1</span>) - X(<span class="number">1</span>,:)*w;</div><div class="line"></div><div class="line">(X*w + b) / norm(w) </div><div class="line"><span class="comment">%predict new points:</span></div><div class="line">yNew = <span class="built_in">sign</span>(w'*[<span class="number">3</span>;<span class="number">-1</span>] + b);</div></pre></td></tr></table></figure><h2 id="Extensions-of-SVMs"><a href="#Extensions-of-SVMs" class="headerlink" title="Extensions of SVMs"></a>Extensions of SVMs</h2><h3 id="Kernel-Function"><a href="#Kernel-Function" class="headerlink" title="Kernel Function"></a>Kernel Function</h3><p>在前面的讨论中，我们认为样本在特征空间是线性可分的，然而在现实任务中，原始样本空间不一定是线性可分的，最常见的例子就是“异或”。对于这样的问题，我们可以考虑从原始空间映射到更高维的特征空间，即将样本“升维”。</p><p>令\(\phi(x)\)为\(x\)升维后的表示，则相应形式的SVM原问题为：</p><p>$$ \min_{w,b} \ \  \frac{1}{2}||w||_{2}^{2}  \\ s.t. \ \ y_i(w^T\phi(x_i) + b) \geq 1$$</p><p>对偶问题为：</p><p>$$\max_{\alpha} L(w,b,\alpha) = -\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_j{\phi(x_i)}^T\phi(x_j) + \sum_{i=1}^{m} \alpha_i$$</p><p>相应的对偶问题的约束条件为：</p><p>$$\alpha_i \geq 0 \\ \sum_{i=1}^{m}\alpha_iy_i = 0$$</p><p>可以发现在上面的等式中存在\({\phi(x_i)}^T\phi(x_j)\)的计算，然而由于升维后的空间维度可能很高，因此直接在升维后的特征空间计算内积通常是很困难的。在这里我们引入核函数(kernel function) 使得\(k(x_i,x_j) = {\phi(x_i)}^T\phi(x_j)\), 即将在升维后空间的内积转化为在原空间通过核函数计算的结果，相应的有：</p><p>$$\max_{\alpha} L(w,b,\alpha) = -\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jk(x_i,x_j) + \sum_{i=1}^{m} \alpha_i$$</p><p>需要注意的是核函数要求其核矩阵是半正定的。常见的核函数有：</p><ul><li>线性核：\(k(x_i,x_j) = x_i^Tx_j\)</li><li>多项式核: \(k(x_i,x_j) = (x_i^Tx_j)^d\)</li><li>高斯核: \(k(x_i,x_j) = \exp(-\frac{||x_i-x_j||^2}{2\sigma^2})\)</li></ul><h3 id="Soft-margin-SVM"><a href="#Soft-margin-SVM" class="headerlink" title="Soft-margin SVM"></a>Soft-margin SVM</h3><p>在前面的讨论中，我们一直假定样本空间是线性可分的，然而现实任务中往往很难确定训练样本在特征空间中是否是线性可分。解决这个问题的办法是引入软间隔的概念(soft-margin)。在之前的问题中，我们认为所有点均需满足\(y_i(w^Tx_i+b) \geq 1\), 这个间隔太硬了，我们称之为硬间隔(hard-margin).</p><p>为了解决硬间隔的问题，我们允许可以有点不满足硬间隔的条件，即\(y_i(w^Tx_i + b) \geq 1 - \xi_i\), 同时要求在最大化margin的同时，不满足硬间隔约束的点尽可能少，即\(\min_{w,b,\xi}\frac{1}{2}w^Tw + C\sum_{i=1}^{m} \xi_i\),综上可得：</p><p>$$\min_{w,b,\xi} \frac{1}{2}w^Tw + C\sum_{i=1}^{m} \xi_i \\ s.t. \ \ y_i(w^Tx_i + b) \geq 1 - \xi_i \\ \xi_i \geq 0$$</p><p>需要提到的一点是，目标函数中\(\sum_{i=1}^{m} \xi_i \) 的引入实际上采用的是hinge loss: \(\max(0,1-z)\).</p><p>对soft-margin SVM求解可以通过求其对偶问题的。soft-margin SVM相应的拉格朗日表达式为：</p><p>$$L(w,b,\alpha,\xi,\mu) = \frac{1}{2}w^Tw + C\sum_{i=1}^{m} \xi_i + \sum_{i=1}^{m}\alpha_i(1-\xi_i-y_i(w^Tx_i+b)) - \sum_{i=1}^{m}\mu_i\xi_i$$</p><p>相应的KKT条件为：</p><p>$$\alpha_i \geq 0, \ \ \ \mu_i \geq 0 \\ y_i(w^Tx_i + b) \geq 1 - \xi_i \\ \xi_i \geq 0 \\ \alpha_i(1-\xi_i-y_i(w^Tx_i+b)) = 0 \\ \mu_i\xi_i = 0$$</p><p>对\(w,b,\xi\)求偏导可得：</p><p>$$w = \sum_{i=1}^{m} \alpha_ix_iy_i $$</p><p>$$\sum_{i=1}^{m}\alpha_iy_i = 0$$</p><p>$$C = \alpha_i + \mu_i$$</p><p>得出的对偶问题形式为：</p><p>$$\max_{\alpha} \sum_{i=1}^{m}\alpha_i -\frac{1}{2} \sum_{i=1}^{m}\sum_{j=1}^{n}\alpha_i\alpha_jy_iy_jx_i^Tx_j \\ s.t. 0 \leq \alpha_i \leq C \\ \sum_{i=1}^{m} \alpha_iy_i = 0$$</p><p>若\(\alpha_i &gt; 0\) 则有 \(y_i(w^Tx_i + b) = 1 - \xi_i\), 相应的样本均为support vectors. 此时若\(\alpha_i &lt; C\), 则\(\mu_i &gt; 0\), \(\xi_i = 0\),可知该样本恰好在margin平面上；若\(\alpha_i ＝ C\), 则\(\mu_i ＝ 0\),此时若\(\xi_i &lt; 1\),可知该样本在最大间隔内部；若\(\xi_i &gt; 1\),可知该样本分类错误。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Book: <a href="prml">Pattern Recognition and Machine Learning by Christopher Bishop</a></li><li><a href="http://www.holehouse.org/mlclass" target="_blank" rel="external">Notes of machine learning by Andrew Ng in cousera</a></li><li><a href="http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm" target="_blank" rel="external">机器学习</a> 周志华</li><li>Slides by <a href="http://sist.shanghaitech.edu.cn/StaffDetail.asp?id=334" target="_blank" rel="external">Prof. Wang</a></li><li><a href="http://www.hangli-hl.com/books.html" target="_blank" rel="external">统计学习方法</a></li></ul><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>]]></content>
    
    <summary type="html">
    
      本文介绍了支持向量机 (Support Vector Machines)的来源、原问题和对偶问题的推倒以及优化，核函数和Soft-margin SVM。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/tags/Machine-Learning/"/>
    
      <category term="Support Vector Machines" scheme="http://frankchu0229.github.io/tags/Support-Vector-Machines/"/>
    
      <category term="SVM" scheme="http://frankchu0229.github.io/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>Exercises</title>
    <link href="http://frankchu0229.github.io/2016/04/26/ML6/"/>
    <id>http://frankchu0229.github.io/2016/04/26/ML6/</id>
    <published>2016-04-25T16:00:00.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Part-I-Data-Preprocessing"><a href="#Part-I-Data-Preprocessing" class="headerlink" title="Part I : Data Preprocessing"></a>Part I : Data Preprocessing</h2><p>在实际数据中，我们总会遇到类别变量(category variables)。对于这种变量，它是没有数值意义上大小的，我们要用dummy coding来编码，然后再来处理。常用的dummy coding方法：对于有n种类别的类别变量，我们用n-1位数来编码。(例如：有四个类别种类的变量，我们用000，001，010，100来编码)。在实际数据中，经常也会出现数据缺失的情况，在这里我是用该属性(attribute)的均值来填充的(当然也可以用其他的办法来解决：比如删除有缺失属性的样本，或者将缺失的属性作为一个新的类别等).</p><h2 id="Part-II-Linear-Regression-and-Stochastic-Gradient-Descent-SGD"><a href="#Part-II-Linear-Regression-and-Stochastic-Gradient-Descent-SGD" class="headerlink" title="Part II: Linear Regression and Stochastic Gradient Descent (SGD)"></a>Part II: Linear Regression and Stochastic Gradient Descent (SGD)</h2><p>这一部分主要贴下minibatch-SGD的实现，关于Linear Regression的实现已经在前几篇博客中提过了。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[paraTheta,cost]</span> = <span class="title">sgd</span><span class="params">(theta0,X,y)</span></span></div><div class="line"></div><div class="line">epochs = <span class="number">50</span>;</div><div class="line">minibatch = <span class="number">256</span>;</div><div class="line">m = <span class="built_in">size</span>(X,<span class="number">1</span>);</div><div class="line">alpha = <span class="number">0.001</span>;</div><div class="line"></div><div class="line">paraTheta = theta0;</div><div class="line"></div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>: epochs</div><div class="line">rp = randperm(m);</div><div class="line"></div><div class="line"><span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> : minibatch : (m - minibatch + <span class="number">1</span>)</div><div class="line">x1 = X(rp(<span class="built_in">j</span>:<span class="built_in">j</span>+minibatch<span class="number">-1</span>)',:);</div><div class="line">y1 = y(rp(<span class="built_in">j</span>:<span class="built_in">j</span>+minibatch<span class="number">-1</span>)');</div><div class="line">[cost,gradient] = linearRegressionCost(paraTheta,x1,y1);</div><div class="line">paraTheta = paraTheta - alpha*gradient;</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line">alpha = alpha/<span class="number">2.0</span>;</div><div class="line">cost = <span class="number">0.5</span>*norm(X*paraTheta-y);</div><div class="line"><span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[cost, gradient]</span> = <span class="title">linearRegressionCost</span><span class="params">(theta,X,y)</span></span></div><div class="line"> cost = <span class="number">0.5</span>*norm(X*theta-y,<span class="number">2</span>);</div><div class="line"> gradient = X'*(X*theta - y);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><h2 id="Part-III-Overfitting-and-Regularization-for-Regression"><a href="#Part-III-Overfitting-and-Regularization-for-Regression" class="headerlink" title="Part III: Overfitting and Regularization for Regression"></a>Part III: Overfitting and Regularization for Regression</h2><p>当我们对模型参数加了L1 norm约束之后(LASSO)，模型的优化目标不再是在全部点处可导，这时basic gradient descent的方法就不再可行了。关于LASSO的一阶最优条件，在 <a href="http://frankchu0229.github.io/2016/04/12/ML4/">Linear Models for Regression</a>这篇博客中已经提过了。常用的求解LASSO的方法有：迭代软阈值算法(IST), 最小角回归(LARS), 坐标下降法(coordinate descent), ADMM 等。</p><h3 id="迭代软阈值算法-IST"><a href="#迭代软阈值算法-IST" class="headerlink" title="迭代软阈值算法(IST)"></a>迭代软阈值算法(IST)</h3><p>在linear regression中，我们的优化目标为\(E_{in} = \frac{1}{2} ||\mathbf{X}\theta - \mathbf{y}||_2^2 + \lambda||\theta||_{1} = f(\theta) + \lambda||\theta||_{1}\). </p><p>IST算法：在点\(\theta\)处，$$ Q(z,\theta) = f(\theta) + \nabla f(\theta)^T(z - \theta) + \frac{1}{2}||z - \theta||_2^2$$ 是对\(f(z)\)的二阶近似。现在考虑我们的L1约束问题：\(\min_{\theta}  f(\theta) + \lambda||\theta||_{1} \)，对第k次迭代，IST用local的\(Q(z,\theta_{k}) + \lambda ||z||_1\)来代替：\( f(\theta) + \lambda||\theta||_{1} \). 这样有：<br>$$\theta_{k+1} = arg \min_{z} Q(z,\theta_{k}) + \lambda ||z||_1 $$</p><p>\(\theta_{k+1}\)的解的形式可以用软阈值算子来写出：\(\theta_{k+1} = S_{\lambda}(\theta_{k} - \nabla f(\theta_{k}^T)) = S_{\lambda}(\theta_{k} - \mathbf{X}^T(\mathbf{X}\theta_{k} - \mathbf{y})\). 其中，</p><div align="center"><br><img src="../img/softThreshold.png" width="200" height="100" alt="softThreshold.png" align="center"><br></div><p>关于软阈值算子的实现：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">y</span> = <span class="title">soft</span><span class="params">(x,tau)</span></span></div><div class="line"><span class="comment">%This is the soft-thresholding operator</span></div><div class="line">y = <span class="built_in">sign</span>(x).*max(<span class="built_in">abs</span>(x)-tau,<span class="number">0</span>);</div></pre></td></tr></table></figure><p>总结：IST是一种迭代优化的方法，对于第k次迭代\(\theta_k\), IST对目标函数在\(\theta_k\)处进行展开，用目标函数的二阶近似来代替该目标函数；通过求解该二阶近似函数来得出第k+1次迭代\(\theta_{k+1}\)的值。</p><h3 id="坐标下降法-Coordinate-Descent"><a href="#坐标下降法-Coordinate-Descent" class="headerlink" title="坐标下降法 (Coordinate Descent)"></a>坐标下降法 (Coordinate Descent)</h3><p>Coordinate descent (CD)是一种常用的求解模型参数的方法。在每一次迭代中，CD不是沿着参数梯度方向进行搜索，而是沿着坐标方向进行搜索。对于有n个参数的linear regression问题，CD在每一次迭代中沿着一个坐标方向去搜索，通过循环使用不同坐标方向来达到目标函数的局部最小值。</p><p>\(E_{in} = \frac{1}{2} ||\mathbf{X}\theta - \mathbf{y}||_2^2 + \lambda||\theta||_{1} = f(\theta) + \lambda||\theta||_{1}\). </p><p>对第i个坐标方向进行最小化,得到一阶最优条件：<br>$$\mathbf{X}^T_{i}(\mathbf{X}\theta - \mathbf{y}) + \lambda s_i = \mathbf{X}^T_{i} \mathbf{X}_i \theta_i + \mathbf{X}^T_{i}\mathbf{X}{-i}\theta_{-i} - \mathbf{X}^T_{i}\mathbf{y} +  \lambda s_i  = 0 $$</p><p>其中，\(\mathbf{X}_i \)表示 \(\mathbf{X}\)的第i列，\(\mathbf{X}_{-i} \)表示\(\mathbf{X}\)除去第i列，\(s_i\)表示次梯度。</p><p>通过以上一阶最优条件解得：</p><p>$$\theta_i = S_{\lambda / ||\mathbf{X}_i||_2^2}(\frac{\mathbf{X}_i^T (\mathbf{y} - \mathbf{X}{-i}\theta_{-i})}{\mathbf{X}_i^T \mathbf{X}_i})$$</p><p>其中，\(i = 1,2,…n\), S为软阈值算子。</p><p>当目标函数为光滑的时候，CD通过沿着不同坐标方向不断迭代，可以收敛到局部极小值或者驻点；当目标函数不光滑的时候，CD可能会收敛到非驻点(non-stationary point).</p><p>下面是对Linear Regression问题的CD解法。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[paraTheta, cost]</span> = <span class="title">cdCost</span><span class="params">(theta0,X,y,lambda)</span></span></div><div class="line"></div><div class="line">theta = theta0;</div><div class="line">epochs = <span class="number">50</span>;</div><div class="line">n = <span class="built_in">size</span>(X,<span class="number">2</span>);</div><div class="line">optTol = <span class="number">0.0001</span>;</div><div class="line"></div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>: epochs</div><div class="line">theta_old = theta;</div><div class="line"><span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:n </div><div class="line">theta(<span class="built_in">j</span>) = softThreshold(<span class="built_in">j</span>,theta,X,y,lambda);</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line">cost = <span class="number">0.5</span>*norm(X*theta-y) + lambda*norm(theta,<span class="number">1</span>)</div><div class="line"><span class="comment">%% checking convergence:</span></div><div class="line"><span class="keyword">if</span> sum(<span class="built_in">abs</span>(theta-theta_old))/sum(<span class="built_in">abs</span>(theta)) &lt; optTol</div><div class="line"><span class="keyword">break</span>;</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="keyword">end</span></div><div class="line">paraTheta = theta;</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[thetai]</span> = <span class="title">softThreshold</span><span class="params">(i,theta,X,y,lambda)</span></span></div><div class="line">Xt = X';</div><div class="line">a = Xt(<span class="built_in">i</span>,:)*y;</div><div class="line">c = X*theta - X(:,<span class="built_in">i</span>)*theta(<span class="built_in">i</span>);</div><div class="line">b = Xt(<span class="built_in">i</span>,:)*c;</div><div class="line">den = Xt(<span class="built_in">i</span>,:)*X(:,<span class="built_in">i</span>);</div><div class="line"></div><div class="line"><span class="keyword">if</span>(a-b &gt; lambda)</div><div class="line">thetai = (a-b-lambda) / den;</div><div class="line"></div><div class="line"><span class="keyword">elseif</span>(a-b &lt; -lambda)</div><div class="line">thetai = (a-b+lambda) / den;</div><div class="line"><span class="keyword">else</span></div><div class="line">thetai = <span class="number">0</span>;</div><div class="line"></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><h2 id="Part-IV-Classification"><a href="#Part-IV-Classification" class="headerlink" title="Part IV: Classification"></a>Part IV: Classification</h2><p> 关于Logistic Regression model的实现已经在<a href="http://frankchu0229.github.io/2016/04/24/ML5/">Linear Models for Classification</a>这篇博客中提过了 (用了gradient descent，Newton method，BFGS 和 l-BFGS(minfunc)四个优化方法来实现)。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Book: <a href="prml">Pattern Recognition and Machine Learning by Christopher Bishop</a></li><li><a href="http://www.holehouse.org/mlclass" target="_blank" rel="external">Notes of machine learning by Andrew Ng in cousera</a></li><li><a href="http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm" target="_blank" rel="external">机器学习</a> 周志华</li><li>Slides by <a href="http://sist.shanghaitech.edu.cn/StaffDetail.asp?id=334" target="_blank" rel="external">Prof. Wang</a></li><li><a href="http://www.hangli-hl.com/books.html" target="_blank" rel="external">统计学习方法</a></li><li><a href="https://www.cs.cmu.edu/~ggordon/10725-F12/slides/25-coord-desc.pdf" target="_blank" rel="external">Coordinate Descent</a></li></ul><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><!--<div  align="center">    <img src="/img/photoswithboyd.jpg" width = "300" height = "200" alt="PhotosWithStephenBoyd" align=center /></div>-->]]></content>
    
    <summary type="html">
    
      本文是对前面涉及到的算法的实践。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/categories/Machine-Learning/"/>
    
      <category term="Exercise" scheme="http://frankchu0229.github.io/categories/Machine-Learning/Exercise/"/>
    
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/tags/Machine-Learning/"/>
    
      <category term="Coordinate Descent" scheme="http://frankchu0229.github.io/tags/Coordinate-Descent/"/>
    
      <category term="BFGS" scheme="http://frankchu0229.github.io/tags/BFGS/"/>
    
      <category term="Exercises" scheme="http://frankchu0229.github.io/tags/Exercises/"/>
    
  </entry>
  
  <entry>
    <title>Linear Models for Classification</title>
    <link href="http://frankchu0229.github.io/2016/04/24/ML5/"/>
    <id>http://frankchu0229.github.io/2016/04/24/ML5/</id>
    <published>2016-04-23T16:10:41.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇写了linear models for regression, 这一篇打算写linear models for classification. 当 \(y_i\) 为连续变量时，通过输入 \(\mathbf{x_i}\) 预测 \(y_i\) 为回归(regression)问题; 当 \(y_i\) 为离散变量时，通过输入 \(\mathbf{x_i}\) 预测 \(y_i\) 为分类 (classification) 问题。</p><h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><p>Logistic regression 是一种常用的分类方法。对于二分类问题，如果要用一个线性分类器去分类的话，最简单的办法就是对输入\(\mathbf{x}\) 做线性组合\(\theta^T \mathbf{x}\), 然后将\(\theta^T \mathbf{x}\)和阈值\(\tau\)进行比较。如果大于该阈值，则为正例；否则为负例。这种和阈值进行比较的方法等效于分段函数。分段函数虽然简单，但它不连续，非凸。</p><p>Sigmoid (logistic) 函数可以作为该分段函数的替代。<br>$$f(\theta^T\mathbf{x}) = \frac{1}{1 + e^{- \theta^T\mathbf{x}}}$$</p><p>这样，当 \(f(\theta^T\mathbf{x}) &gt; \frac{1}{2}\)时，我们认为\(\mathbf{x}\)属于正类；当 \(f(\theta^T\mathbf{x}) &lt; \frac{1}{2}\)时，我们认为\(\mathbf{x}\)属于负类。我们把用sigmoid函数的这种方法称为 logistic regression。</p><h2 id="关于Logistic-Regression的由来"><a href="#关于Logistic-Regression的由来" class="headerlink" title="关于Logistic Regression的由来:"></a>关于Logistic Regression的由来:</h2><p>我们对 \(f(\theta^T\mathbf{x}) = \frac{1}{1 + e^{- \theta^T\mathbf{x}}} \) 进行变换可以得到：\(\log \frac{f(\theta^T\mathbf{x})}{1-f(\theta^T\mathbf{x})} = \theta^T\mathbf{x}\). 其中，\( \frac{f(\theta^T\mathbf{x})}{1-f(\theta^T\mathbf{x})}\) 称为几率(odd ratio); \(\log \frac{f(\theta^T\mathbf{x})}{1-f(\theta^T\mathbf{x})}\)称为对数几率 (log odds,or logit)。 可以看到，\(\log \frac{f(\theta^T\mathbf{x})}{1-f(\theta^T\mathbf{x})} = \theta^T\mathbf{x}\) 实际上是用输入的线性组合\(\theta^T\mathbf{x}\)来预测对数几率 \(\log \frac{f(\theta^T\mathbf{x})}{1-f(\theta^T\mathbf{x})}\)，所以把这种方法称为对数几率回归 Logit Regression (Logistic Regression).</p><h2 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h2><p>关于Logistic Regression的cost function，最先想到的应该是MSE，但是我们通常所见到的cost function都是由MLE的方式推出来的。一个不用MSE的原因是：当\(\theta^T\mathbf{x}\)接近\(\frac{1}{2}\)时，\(y_i - f(\theta^T\mathbf{x_i}) \)的差值会很大，使得用MSE得到的cost function对actual loss(error rate)的近似效果比较差。(<a href="http://stackoverflow.com/questions/12157881/cost-function-for-logistic-regression" target="_blank" rel="external">More</a>).</p><p>那么不用MSE的话，怎么来定cost function呢？ 可以发现\(f(\theta^T\mathbf{x})\)的取值在\(0 \sim 1\), 我们可以令<br>$$f(\theta^T\mathbf{x}) = p(y = 1|\mathbf{x} )\\ 1-f(\theta^T\mathbf{x}) = p(y = 0|\mathbf{x})$$<br>这里\(y \in \{0,1\}\), 则可以得到 $$p(y|\mathbf{x}) = f(\theta^T\mathbf{x})^{y}(1-f(\theta^T\mathbf{x}))^{1-y}$$</p><p>相应的 Likelihood 为：<br>$$ L = \prod_{i = 1}^{N} p(y_i|\mathbf{x_i})$$</p><p>通过取负对数并展开，得到cost function为：<br>$$E_{in} = - \frac{1}{N}\sum_{i = 1}^{N} y_i \log f(\theta^T\mathbf{x_i}) + (1 - y_i)\log (1-f(\theta^T\mathbf{x_i}))$$</p><p>这里对参数\(\theta\) 的学习，我们可以使用梯度下降法来得到：<br>$$\nabla E_{in} = -\frac{1}{N} \sum_{i=1}^{N}(y_i - f(\theta^T\mathbf{x_i}))\mathbf{x_i} \\ \theta := \theta - \alpha \nabla E_{in}$$</p><h2 id="正则化："><a href="#正则化：" class="headerlink" title="正则化："></a>正则化：</h2><p>为了防止过拟合，通常的做法是在优化目标中加入惩罚项，通过惩罚过大的参数来防止过拟合。常用的正则化有：L1范数和L2范数。L1正则化倾向于使参数变为0，所以可以得到更稀疏的解。 增加L1范数的优化目标：</p><p>$$E_{in} = - \frac{1}{N}\sum_{i = 1}^{N} y_i \log f(\theta^T\mathbf{x_i}) + (1 - y_i)\log (1-f(\theta^T\mathbf{x_i})) ＋ \lambda ||\theta||_1$$</p><p>需要注意的是\(||\theta||_1\)不是在所有点处都是可导的，所以这里用不了通常的gradient descent方法，这里可以用Proximal Gradient Descent (PGD) 方法来求解。 关于PGD的方法暂时不在这里介绍，会在后面关于sparse的文章里面进行介绍。</p><h1 id="多分类："><a href="#多分类：" class="headerlink" title="多分类："></a>多分类：</h1><p>当y不是在 \(\{0,1\}\)中取值，而是可能从K个类别中取值时，我们就得到了多(K)分类问题。对于多分类任务，我们可以通过“拆解法”来解决，即将多分类任务拆分为若干个二分类任务来解决。常用的拆分策略有：一对一(OvO), 一对其余(OvR), 多对多(MvM).(更多有关“拆解法”可以参考<a href="url1">周志华老师新书第三章</a>)。</p><p>关于多分类任务，更常见的是用 softmax regression来解决。Softmax regression 可以看作是logistic regression在多分类任务上的推广。关于具体怎么推广的，这里不再介绍，有兴趣的读者可以通过 K-1 个\(\ln\frac{p(y = i)}{p(y = K)} = \theta_i^T\mathbf{x}\)等式来推倒(i = 1,2,…K-1)。</p><p>Softmax regression用softmax函数来对概率进行建模，具体形式如下：<br>$$p(y = i|\mathbf{x},\theta) = \frac{e^{\theta_i^T\mathbf{x}}}{\sum_{j = 1}^{K} e^{\theta_j^T\mathbf{x}}}$$</p><p>相应的有：<br>$$ p(y|\mathbf{x},\theta) = \prod_{i = 1}^{K} p(y = i|\mathbf{x},\theta)^{\mathbb{1}(y = i)} $$</p><p>对应的Likelihood：<br>$$L = \prod_{j = 1}^{N}  \prod_{i = 1}^{K} p(y_j = i|\mathbf{x_j},\theta)^{\mathbb{1}(y_j = i)}  $$</p><p>对应的cost function为：<br>$$E_{in} = -\frac{1}{N} \sum_{j = 1}^{N}\sum_{i = 1}^{K} \mathbb{1}(y_j = i) \ln  \frac{e^{\theta_i^T\mathbf{x_j}}}{\sum_{l = 1}^{K} e^{\theta_l^T\mathbf{x_j}}} $$</p><p>这里对参数θ 的学习，我们可以使用梯度下降法来得到：<br>$$\frac{\partial E_{in} }{\partial \theta_i}  = -\frac{1}{N} \sum_{j =1}^{N} \mathbf{x_j}(\mathbb{1}(y_j = i) -  \frac{e^{\theta_i^T\mathbf{x_j}}}{\sum_{j = 1}^{K} e^{\theta_j^T\mathbf{x_j}}}) $$</p><p>关于对 softmax函数求导：<br>$$\frac{\partial f(x_i)}{x_i} = f(x_i)(1-f(x_i)) \\ \frac{\partial f(x_i)}{x_j} = -f(x_i)f(x_j) $$</p><p>其中，\(f(x_i) = \frac{e^{x_i}}{\sum_{i = 1}^{K}e^{x_i}}\).</p><p>由于softmax regression中参数是冗余的，这样就会使得有多组参数满足最优条件。在实际操作中，通常在优化目标基础上加上weight decay(对参数的L2正则化)来解决这一问题。</p><p>当K个类别是互斥的，适合使用softmax regression来做多分类任务；当K个类别不是互斥的，即一个\(\mathbf{x}\)可以属于多个类别，这个时候适合使用K次OvR logistic regression来做分类。</p><h1 id="一个简单的-Logistic-Regression-例子："><a href="#一个简单的-Logistic-Regression-例子：" class="headerlink" title="一个简单的 Logistic Regression 例子："></a>一个简单的 Logistic Regression 例子：</h1><p>用minfunc优化包来解决：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">clc;clear;</div><div class="line">D = [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span>; <span class="number">2</span> <span class="number">2</span> <span class="number">0</span>;<span class="number">2</span> <span class="number">0</span> <span class="number">1</span>; <span class="number">3</span> <span class="number">0</span> <span class="number">1</span>];</div><div class="line">X = [ones(size(D,<span class="number">1</span>),<span class="number">1</span>) D(:,<span class="number">1</span>:<span class="number">2</span>)];</div><div class="line">y = D(:,<span class="number">3</span>);</div><div class="line">w = <span class="built_in">zeros</span>(<span class="number">3</span>,<span class="number">1</span>);</div><div class="line">gNorm = Inf;</div><div class="line">k = <span class="number">1</span>;</div><div class="line"></div><div class="line"><span class="comment">%% using minFunc :</span></div><div class="line">addpath minFunc_2012/minFunc/</div><div class="line">options = struct;</div><div class="line">options.maxIter = <span class="number">400</span>;</div><div class="line">options.Method  = <span class="string">'lbfgs'</span>;</div><div class="line">options.useMex = <span class="number">0</span>;</div><div class="line">minFuncOptions.display = <span class="string">'on'</span>;</div><div class="line"></div><div class="line">[paraTheta, cost] = minFunc(@(w) logisticCost(w,X,y),w,options);</div></pre></td></tr></table></figure><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[cost gradient]</span> = <span class="title">logisticCost</span><span class="params">(w,X,y)</span></span></div><div class="line">m = <span class="built_in">size</span>(y,<span class="number">1</span>);</div><div class="line">cost = <span class="number">0</span>;</div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> =<span class="number">1</span>:m</div><div class="line">    cost =  cost - (y(<span class="built_in">i</span>)*<span class="built_in">log</span>(sigmoid(X(<span class="built_in">i</span>,:),w)) + (<span class="number">1</span>- y(<span class="built_in">i</span>))*<span class="built_in">log</span>(<span class="number">1</span>- sigmoid(X(<span class="built_in">i</span>,:),w))) ;</div><div class="line"><span class="keyword">end</span></div><div class="line">cost = cost / m;</div><div class="line">gradient = X'*(sigmoid(X,w)-y);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>用Gradient Descent来解决：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">%% gradient descent</span></div><div class="line">g_GD = <span class="built_in">zeros</span>(<span class="number">100</span>,<span class="number">1</span>);</div><div class="line"><span class="keyword">while</span>(gNorm &gt; <span class="number">1e-5</span>)     </div><div class="line">g = X'*(sigmoid(X,w)-y);</div><div class="line">w = w - g;</div><div class="line">gNorm = norm(g,<span class="number">2</span>);</div><div class="line">g_GD(k) = gNorm;</div><div class="line">k = k + <span class="number">1</span>;</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>用牛顿法解决：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span>(gNorm &gt; <span class="number">1e-5</span>)    </div><div class="line">g = X'*(sigmoid(X,w)-y);</div><div class="line">S = <span class="built_in">diag</span>(sigmoid(X,w).*(<span class="number">1</span>-sigmoid(X,w)));</div><div class="line">H = X'*S*X;</div><div class="line">w = w - inv(H)*g;</div><div class="line">gNorm = norm(g,<span class="number">2</span>);</div><div class="line">g_NT(k) = gNorm;</div><div class="line">k = k + <span class="number">1</span>;</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line">wrongCnt = prediction(w,X,y);</div><div class="line">plot(<span class="built_in">log</span>(g_NT));</div></pre></td></tr></table></figure><p>用拟牛顿法解决：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment">%% BFGS method:</span></div><div class="line"></div><div class="line">g = X'*(sigmoid(X,w)-y);</div><div class="line">B = <span class="built_in">eye</span>(<span class="number">3</span>,<span class="number">3</span>);</div><div class="line">I = <span class="built_in">eye</span>(<span class="number">3</span>,<span class="number">3</span>);</div><div class="line"></div><div class="line"><span class="keyword">while</span> norm(g) &gt; <span class="number">1e-5</span></div><div class="line">   s = -B * g; </div><div class="line">   w = w + s; </div><div class="line">   g_new = X'*(sigmoid(X,w)-y);</div><div class="line">   z = g_new - g;</div><div class="line">   g = g_new;       </div><div class="line">   B = (I - s * z' ./ (z' * s)) * B * ...</div><div class="line">   (I - z * s' ./ (z' * s)) + s * s' ./ (z' * s);</div><div class="line">   g_BFGS(k) = norm(g);         </div><div class="line">   k = k + <span class="number">1</span>;</div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line">wrongCnt = prediction(w,X,y);</div><div class="line">plot(<span class="built_in">log</span>(g_BFGS));</div></pre></td></tr></table></figure><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span>  <span class="title">y</span> = <span class="title">sigmoid</span><span class="params">(X,w)</span></span></div><div class="line">y = <span class="number">1</span> ./ (<span class="number">1</span> + <span class="built_in">exp</span>(-X*w)); </div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[wcnt]</span> = <span class="title">prediction</span><span class="params">(w,X,y)</span></span></div><div class="line">y1 = sigmoid(X,w);</div><div class="line">yNew = y1 &gt; <span class="number">0.5</span>;</div><div class="line">wcnt = sum(yNew ~= y);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Book: <a href="prml">Pattern Recognition and Machine Learning by Christopher Bishop</a></li><li><a href="http://www.holehouse.org/mlclass" target="_blank" rel="external">Notes of machine learning by Andrew Ng in cousera</a></li><li><a href="http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm" target="_blank" rel="external">机器学习</a> 周志华</li><li>Slides by <a href="http://sist.shanghaitech.edu.cn/StaffDetail.asp?id=334" target="_blank" rel="external">Prof. Wang</a></li><li><a href="http://www.hangli-hl.com/books.html" target="_blank" rel="external">统计学习方法</a></li><li><a href="http://www.di.ens.fr/~fbach/mlss08_fbach.pdf" target="_blank" rel="external">Learning with sparsity-inducing norms</a></li><li><a href="http://www.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture18.pdf" target="_blank" rel="external">Proximal gradient method</a></li><li><a href="http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression" target="_blank" rel="external">softmax ufldl</a></li></ul><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>]]></content>
    
    <summary type="html">
    
      本文介绍了Logistic Regression 和用于多分类任务上的softmax。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/categories/Machine-Learning/"/>
    
      <category term="Logistic Regression" scheme="http://frankchu0229.github.io/categories/Machine-Learning/Logistic-Regression/"/>
    
      <category term="Softmax" scheme="http://frankchu0229.github.io/categories/Machine-Learning/Logistic-Regression/Softmax/"/>
    
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/tags/Machine-Learning/"/>
    
      <category term="Logistic Regression" scheme="http://frankchu0229.github.io/tags/Logistic-Regression/"/>
    
      <category term="Softmax" scheme="http://frankchu0229.github.io/tags/Softmax/"/>
    
  </entry>
  
  <entry>
    <title>Linear Models for Regression</title>
    <link href="http://frankchu0229.github.io/2016/04/12/ML4/"/>
    <id>http://frankchu0229.github.io/2016/04/12/ML4/</id>
    <published>2016-04-12T13:17:18.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>线性模型：模型输出对模型参数成线性。线性模型可以看作是一组输入变量非线性映射的线性组合。线性模型比较简单，通常在介绍一些较复杂的模型之前，先介绍线性模型作为基础；线性模型也常用作解释机器学习中一些现象的例子 (e.g., curse of dimensionality)。</p><p>令 \( D = \{(\mathbf{x_1},y_1),(\mathbf{x_2},y_2)…,(\mathbf{x_m},y_m)\}\) 为数据集，其中 \(\mathbf{x} = [x_1;x_2;…;x_n]\) 为n维列向量。当 \(y_i\) 为连续变量时，通过输入 \(\mathbf{x_i}\) 预测 \(y_i\) 为回归(regression)问题; 当 \(y_i\) 为离散变量时，通过输入 \(\mathbf{x_i}\) 预测 \(y_i\) 为分类 (classification) 问题。</p><h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h1><p>线性回归(linear regression) 试图学到：<br>$$ f(\mathbf{x}) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + … + \theta_n x_n$$<br>用矩阵的形式表示为：<br>$$f(\mathbf{x}) = \mathbf{x}^T \mathbf{\theta} $$<br>其中，\(\mathbf{\theta} = [\theta_0;\theta_1;\theta_2;…;\theta_n]\); \(\mathbf{x} = [1,\mathbf{x}^T]\);  \(f(\mathbf{x})\)为\(\mathbf{x}\)的预测值。</p><p>线性回归的cost function为均方误差MSE：<br>$$J(\theta) = \frac{1}{2} (\mathbf{X} \mathbf{\theta}  - \mathbf{y})^T( \mathbf{X} \mathbf{\theta} - \mathbf{y}) = \frac{1}{2}||\mathbf{X} \mathbf{\theta}  - \mathbf{y}||_2^2$$<br>其中，\(\mathbf{X} = [\mathbf{x_1}; \mathbf{x_2};… \mathbf{x_m}]\), \(\mathbf{x_i} 为加入1之后的向量\)。</p><p>我们将基于均方误差最小化来求解模型的方法称为“最小二乘法” (least square method).</p><p>线性回归的cost function为凸函数，我们可以直接对其求导来得到最优的\(\theta\)。<br>$$ \frac{\partial J(\theta)}{\partial \theta} = \mathbf{X}^T(\mathbf{X}\theta - \mathbf{y})$$<br>从而得到 \(\theta^* = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\), 其中 \((\mathbf{X}^T\mathbf{X})^{-1}\) 为矩阵\(\mathbf{X}^T\mathbf{X}\)的逆矩阵。</p><p>然而，当数据\(\mathbf{x}\) 的维度很高时，\((\mathbf{X}^T\mathbf{X})^{-1}\) 求解需要很长时间。更常用的方法是梯度下降法 (gradient descent).</p><h1 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h1><p>函数值沿着负梯度方向下降最快，梯度下降法就是基于这一点的：</p><p>$$\theta := \theta - \alpha \nabla J(\theta)$$</p><p>其中\(\alpha\)为学习率(learning rate)，用来控制每一步沿着负梯度方向走的步伐的大小。\(\alpha\)既不可太大，也不可太小。当\(\alpha\)太小时，要迭代很多次，运行时间过长；当\(\alpha\)太大时，可能会发生震荡，找不到(局部)最优值。梯度下降法是常用的一阶优化方法，但是需要对学习率进行调参。</p><p>根据计算梯度时所用的数据数量，gradient descent有三种变化：</p><ul><li><strong>Batch Gradient Descent</strong>: $$\theta := \theta - \alpha \nabla J(\theta)$$ Batch gradient descent使用全部数据计算梯度, 所以更新一次参数\(\theta\)需要很长的时间。并且，batch gradient descent不能进行online learning。Batch gradient descent 能够保证收敛到全局最小值(目标函数为凸函数)或者局部最小值(目标函数为非凸函数)。</li><li><strong>Stachastic Gradient Descent (SGD):</strong><br>$$\theta := \theta - \alpha \nabla J(\theta;\mathbf{x}_i,y_i)$$<br>Stachastic gradient descent 每次使用一个数据去更新参数，使得参数更新很快，并且SGD可用作online learning。由于SGD更新的太过频繁，引入了过高的variance，使得目标函数\(J(\theta)\)波动较大 (<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank" rel="external">见图</a>)。</li><li><strong>Mini-batch Gradient Descent:</strong> $$\theta := \theta - \alpha \nabla J(\theta;\mathbf{x}_{i:i+n},y_{i+n})$$<br>Mini-batch gradient descent 使用n个数据更新参数，减小了SGD更新参数时的variance，达到更稳定的收敛。Mini-batch gradient descent是训练神经网络的一个最常用选择。</li></ul><p>关于更多的gradient descent optimization algorithm (momenterm,Adagrad…) 暂时不在这篇博客里写了，感兴趣的可以先看下这一篇 <a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="external">An overview of gradient descent optimization algorithms </a>.</p><h1 id="Newton-Method"><a href="#Newton-Method" class="headerlink" title="Newton Method"></a>Newton Method</h1><p>当目标函数二阶连续可微时，可以使用牛顿法。牛顿法的迭代次数远小于梯度下降法。</p><p>在高数中，牛顿法是求解方程的一种近似迭代解法。例如在求解方程 \(f(x) = 0\) 时，当用求根公式求不出的时候，可以用牛顿法。对 \(f(x)\) 在 \(x_0\) 处泰勒展开得到： \(f(x) = f(x_0) + (x - x_0) \nabla f(x_0)\), 进而有 \(x = x_0 - \frac{f(x_0)}{\nabla f(x_0)}\)。 我们可以根据  \(x = x_0 - \frac{f(x_0)}{\nabla f(x_0)}\) 进行迭代来求出方程的解。</p><p>在优化问题中，我们想知道的是方程 \(\nabla f(x) = 0\) 的解。当函数二阶连续可微时，我们可以得到方程 \(\nabla f(x) = 0\) 的迭代公式： \(x = x_0 - \frac{\nabla f(x_0)}{\nabla^2 f(x_0)}\)。应用在线性回归问题上即为：<br>$$ \theta := \theta - \frac{\nabla J(\theta)}{\nabla^2 J(\theta)} := \theta - H(J(\theta))^{-1}\nabla J(\theta)$$</p><p>其中，\(H(J(\theta))^{-1}\)为海森矩阵的逆矩阵，计算复杂度相当高，在高维问题中几乎不可行。拟牛顿法用较低的计算代价来寻找海森矩阵逆矩阵的近似矩阵，从而可显著性的降低计算开销。比较常用的拟牛顿法是l-bfgs算法。<a href="https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html" target="_blank" rel="external">minFunc</a> 是机器学习中一个常用的matlab优化包, 只需要自己实现cost和梯度，用minFunc实现线性回归的代码如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">addpath minFunc_2012/minFunc/</div><div class="line">x = <span class="number">1</span>:<span class="number">1</span>:<span class="number">12</span>;</div><div class="line">x = x';</div><div class="line">y = [<span class="number">0.74</span> <span class="number">1.41</span> <span class="number">1.45</span> <span class="number">1.49</span> <span class="number">1.21</span> <span class="number">1.31</span> <span class="number">1.68</span> <span class="number">2.04</span> <span class="number">1.58</span> <span class="number">1.23</span> <span class="number">1.50</span> <span class="number">1.67</span> ]*<span class="number">0.01</span>;</div><div class="line">y = y';</div><div class="line"></div><div class="line"><span class="comment">%% using minFunc :</span></div><div class="line">options = struct;</div><div class="line">options.maxIter = <span class="number">400</span>;</div><div class="line">options.Method  = <span class="string">'lbfgs'</span>;</div><div class="line">options.useMex = <span class="number">0</span>;</div><div class="line">minFuncOptions.display = <span class="string">'on'</span>;</div><div class="line">theta0 = [<span class="number">0</span>; <span class="number">0</span>];</div><div class="line">X = [ones(size(x)) x];</div><div class="line">[paraTheta, cost] = minFunc(@(p) linear_regression(p,X,y),theta0,options);</div><div class="line">yNew = X*paraTheta;</div></pre></td></tr></table></figure><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="params">[cost, gradient]</span> = <span class="title">linear_regression</span><span class="params">(theta,X,y)</span></span></div><div class="line"> cost = <span class="number">0.5</span>*norm(X*theta-y,<span class="number">2</span>);</div><div class="line"> gradient = X'*(X*theta - y);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><h1 id="Linear-Basis-Function-Models"><a href="#Linear-Basis-Function-Models" class="headerlink" title="Linear Basis Function Models"></a>Linear Basis Function Models</h1><p>对于一般的线性模型有：<br>$$ f(\mathbf{x}) = \theta_0 + \sum_{j = 1}^{M-1} \theta_j \phi_j(\mathbf{x}) = \phi(\mathbf{x})\theta $$</p><p>其中，\(\phi(\mathbf{x})\) 为基函数，常用的基函数有：多项式函数、高斯函数、sigmoid函数。相应的cost function为：</p><p>$$J(\theta) = \frac{1}{2} ||\Phi \theta - \mathbf{y}||_2^2$$</p><p>相应的最优解 \(\theta^{*} = (\Phi^T \Phi)^{-1} \Phi^T \mathbf{y}\).</p><h1 id="Ridge-Regression"><a href="#Ridge-Regression" class="headerlink" title="Ridge Regression"></a>Ridge Regression</h1><p>以多项式回归为例，当模型变得更复杂 (所用的\(x^{j}\)阶数越高)时，会出现overfitting的情况，可以发现此时相应的参数\(\theta_i\)会非常大。通常用来控制这种overfitting的做法是，对参数加一个限制：\(\theta^T\theta \leq C\).</p><p>这样就得到了如下的 ridge regression:</p><p>$$ \min_{\theta} \ J(\theta) = \frac{1}{2} ||\Phi \theta - \mathbf{y}||_2^2 \\ s.t. \ \theta^T\theta \leq C$$.</p><p>上式等价于：<br>$$ \min_{\theta} \ J(\theta) = \frac{1}{2} ||\Phi \theta - \mathbf{y}||_2^2 ＋\frac{\lambda}{2} \theta^T\theta $$.</p><p>Ridge regression 是一个典型的凸优化问题，可以通过凸优化包来解决。</p><h1 id="Lasso"><a href="#Lasso" class="headerlink" title="Lasso"></a>Lasso</h1><p>当我们用1-范数对参数\(\theta\)进行约束时，就得到了LASSO：</p><p>$$ \min_{\theta} \ J(\theta) = \frac{1}{2} ||\Phi \theta - \mathbf{y}||_2^2 \\ s.t. \ ||\theta||_1 \leq C$$.</p><p>上式等价于：<br>$$ \min_{\theta} \ J(\theta) = \frac{1}{2} ||\Phi \theta - \mathbf{y}||_2^2 ＋\lambda ||\theta||_1 $$.</p><h2 id="Optimality-Condition-for-LASSO"><a href="#Optimality-Condition-for-LASSO" class="headerlink" title="Optimality Condition for LASSO:"></a>Optimality Condition for LASSO:</h2><p>对 \(J(\theta) = \frac{1}{2} ||\Phi \theta - \mathbf{y}||_2^2 ＋\lambda ||\theta||_1 \) 求导时，我们发现第一项是可导的，但是第二项在\(\theta_i\)为0时，不可导。这里需要引入次微分和次梯度来解决。次微分和次梯度是对导数的一个推广(不可导的情况)，通常用在凸优化中。</p><p>对一个凸函数\(f\)有 \(f(y) \geq f(x) + \nabla f(x)^T(y-x)\), 则 \(g\) is a subgradient of a convex function \(f\) at \(x \in D(f)\) if :<br>$$ f(y) \geq f(x) + g^T (y-x)$$</p><p>次微分(subderivative)是所有满足 \(f(y) \geq f(x) + g^T (y-x)\) 的\(g\) 形成的集合\(\partial f(x)\)。对于非光滑凸函数，一阶最优性条件为：\(0 \in \partial f(x)\) 或者  \(\exists g \in \partial f(x), \ s.t. \ g = 0\)。(即对于光滑可导函数，我们将导数为零作为一阶最优条件；对于非光滑凸函数，我们将0是否在次微分集合作为一阶最优条件)。</p><p>回到LASSO这个问题上，我们可以得到其一阶最优化条件：<br>$$0 \in \Phi^T(\Phi\mathbf{X} - \mathbf{y}) + \lambda \partial ||\theta||_1$$</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Book: <a href="prml">Pattern Recognition and Machine Learning by Christopher Bishop</a></li><li><a href="http://www.holehouse.org/mlclass" target="_blank" rel="external">Notes of machine learning by Andrew Ng in cousera</a></li><li><a href="http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm" target="_blank" rel="external">机器学习</a> 周志华</li><li>Slides by <a href="http://sist.shanghaitech.edu.cn/StaffDetail.asp?id=334" target="_blank" rel="external">Prof. Wang</a></li><li><a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="external">Sebastian Ruder’s blog - An overview of gradient descent optimization algorithms </a></li><li><a href="http://www.hangli-hl.com/books.html" target="_blank" rel="external">统计学习方法</a></li><li><a href="https://en.wikipedia.org/wiki/Subderivative" target="_blank" rel="external">subderative</a></li><li><a href="https://see.stanford.edu/materials/lsocoee364b/01-subgradients_notes.pdf" target="_blank" rel="external">subgradients by Stephen Boyd</a></li></ul><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>]]></content>
    
    <summary type="html">
    
      本文介绍了Linear Regression 和常用的正则化方法以及常用的优化方法。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/tags/Machine-Learning/"/>
    
      <category term="Linear Regression" scheme="http://frankchu0229.github.io/tags/Linear-Regression/"/>
    
      <category term="Gradient Descent" scheme="http://frankchu0229.github.io/tags/Gradient-Descent/"/>
    
      <category term="Newton Method" scheme="http://frankchu0229.github.io/tags/Newton-Method/"/>
    
      <category term="Ridge Regression" scheme="http://frankchu0229.github.io/tags/Ridge-Regression/"/>
    
      <category term="Lasso" scheme="http://frankchu0229.github.io/tags/Lasso/"/>
    
  </entry>
  
  <entry>
    <title>Model Selection and Evaluation</title>
    <link href="http://frankchu0229.github.io/2016/04/07/ML3/"/>
    <id>http://frankchu0229.github.io/2016/04/07/ML3/</id>
    <published>2016-04-06T16:03:07.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>前面两篇分别写了机器学习的简介和一些必备的基础知识，这一篇打算写下模型评估和选择。</p><h1 id="经验误差和过拟合"><a href="#经验误差和过拟合" class="headerlink" title="经验误差和过拟合"></a>经验误差和过拟合</h1><p>我们把学习器实际预测输出和样本真实输出之间的差异称为误差。学习器在训练集上的误差称为经验误差；在新样本上的误差称为泛化误差。显然，我们希望得到泛化误差小的学习器。然而，在现实中，我们并不知道新的样本什么样，能做的只是去最小化经验误差。</p><p>我们希望得到泛化能力强的学习器，即希望学习器在训练样本中学到适合所有潜在样本的“普遍规律”。然而，学习器学习能力过强时，会将训练样本自身拥有的一些特点当作是适合所有潜在样本的规律，这样在新样本上的泛化误差必然会变大。这种现象叫做过拟合。与过拟合相对的是欠拟合，欠拟合一般是由于学习器学习能力太弱导致的。造成过拟合的因素有很多，比较常见的是学习器学习能力过强，而样本相对太少；这样学习器就会学到训练样本中不太一般的特性。过拟合往往是不可避免的，但是可以缓解。暂时打算在后面维度诅咒的博客里，举一些过拟合的例子。</p><h1 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h1><p>在现实任务中，对于同一个问题，我们可以选择很多种学习算法；甚至对于同一个学习算法，当选择参数不同的时候，就会产生不同的模型。我们该选择哪一种学习算法，哪一种参数配置呢？这时就要用到模型选择了 (Model Selection).</p><p>通常我们拿到一个新的数据集，先将数据集 \(D\) 的30%(不超过\(\frac{1}{3}\))用作测试集(test set)，然后再用剩下的数据\(D_{1}\) 来训练和选择模型(即用做训练集(training set)和验证集(validation set)). 对于训练集和验证集的划分，在不同情况下，有不同的适合划分的方法。</p><h2 id="留出法"><a href="#留出法" class="headerlink" title="留出法"></a>留出法</h2><p>留出法直接将 \(D_{1}\) 划分为两个互斥的集合，其中一个集合作为训练集，一个作为验证集。需要注意的是，划分数据集时应尽量保持数据的分布一致性。不然会因为划分数据的过程引入额外的误差从而对最终结果产生影响。比如二分类问题，\(D_{1}\) 有1000个样本，其中正例500个，负例500个。我们现在选70%作为训练集，30%作为验证集。为了保持样本数据分布一致性，我们从正例中取70%负例中取70%作为训练数据。正例和负例中剩下的部分作为验证集。如果从采样角度来看数据划分的话，保留类别比例的采样方式通常称为分层采样。</p><p>留出法需要对数据集\(D_{1}\)进行划分，这就会导致一种窘境：如果训练集S包括绝大多数样本，则训练出的模型可能更接近用\(D_{1}\)训练出的模型，但由于验证集V样本数量过少，评估结果可能不够稳定准确。如果验证集V多包括一些样本，则用训练集S训练出的模型和用\(D_{1}\)训练出的模型可能有较大的差别。这个问题没有十分完美的解决方案，常见的做法就是用大约\(\frac{2}{3} \sim \frac{4}{5}\)样本作为训练集，其他用做验证集。留出法的好处在于简单，而且只需要训练一次模型；缺点在于上面提到的窘境。</p><h2 id="交叉验证法-Cross-Validation"><a href="#交叉验证法-Cross-Validation" class="headerlink" title="交叉验证法 (Cross Validation)"></a>交叉验证法 (Cross Validation)</h2><p>交叉验证法先将数据集\(D_{1}\)划分为k个互斥的子集，并且每个子集尽可能保持数据分布的一致性(从\(D_{1}\)中分层采样)。然后每次用k－1个子集作为训练集，用剩下的作为验证集。这样我们就得到了K组训练集/测试集。最终我们把k个验证集评估结果的平均值来作为该模型的评估值。交叉验证的稳定性和保真性(fidelity)很大程度上取决于k的取值。为了突出这一点，通常又称为k折交叉验证(k-fold cross validation)。 k最常用的取值是10.</p><p>当k等于\(D_{1}\)样本个数m的时候，就得到交叉验证的一个特例：留一法 (LOOCV). 留一法每次只有一个样本作为验证集，这样就免除了随机划分样本方式的影响。同时，留一法每次用m－1个样本来训练数据，使得训练出的模型和在\(D_{1}\)上训练出的模型很相似。但是，留一法每次要跑m次模型学习和预测的算法，当m比较大或者模型的复杂度比较高时，运行时间是很长的。通常留一法用在训练数据比较少的情况。</p><h1 id="性能度量-Evaluation-："><a href="#性能度量-Evaluation-：" class="headerlink" title="性能度量 (Evaluation)："></a>性能度量 (Evaluation)：</h1><p>性能度量是衡量模型泛化能力的评价标准。 在对比不同模型的能力时，选择不同的性能度量，往往会导致不同的评判结果。</p><h2 id="均方误差-Mean-Square-Error"><a href="#均方误差-Mean-Square-Error" class="headerlink" title="均方误差 (Mean Square Error):"></a>均方误差 (Mean Square Error):</h2><p>回归问题常用的性能度量是均方误差(MSE):<br>$$E(f,D) = \frac{1}{m} \sum_{i = i}^{m} (f(\mathbf{x_i}) - y_i)^2$$</p><p>其中，\(D = \{(\mathbf{x_1},y_1),(\mathbf{x_2},y_2),…(\mathbf{x_m},y_m)\}\) 为训练集，\(y_i\)为样例\(\mathbf{x_i}\)的label。\(f\) 为我们学到的模型(预测函数)。 均方误差评估的是学习器 \(f\) 的预测结果和真实结果差异。</p><p>下面介绍分类任务中常用的性能度量。</p><h2 id="错误率与精度："><a href="#错误率与精度：" class="headerlink" title="错误率与精度："></a>错误率与精度：</h2><p>错误率和精度是分类任务中经常用到的性能度量标准，即可用在二分类任务上，也可用在多分类任务上。<br>错误率是分类错误的样本占总样本数的比例，精度是分类正确的样本占样本数的比例，错误率＝ 1 - 精度。用公式表示就是：<br>$$E(f;D) = \sum_{i=1}^{m} \mathbb{1}(f(x_i) \neq y_i)$$<br>$$acc(f;D) = \sum_{i=1}^{m} \mathbb{1}(f(x_i) = y_i)$$</p><h2 id="准确率-Precision-、召回率-Recall-与-F1："><a href="#准确率-Precision-、召回率-Recall-与-F1：" class="headerlink" title="准确率 (Precision)、召回率 (Recall) 与 F1："></a>准确率 (Precision)、召回率 (Recall) 与 F1：</h2><p>错误率和精度虽然很常用，但是并不能满足所有的任务需求。比如在信息检索，web搜索应用中，我们想知道“检索出的信息中有多少比例是用户感兴趣的”(准确率)，或者“在用户感兴趣的信息中，有多少被检索出来”(召回率)。</p><p>下面以一个例子说明，precision 和 recall 的具体计算公式。对应检索信息和用户兴趣这个例子，有如下混淆矩阵：</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">用户感兴趣(真实情况)</th><th style="text-align:center">用户不感兴趣(真实情况)</th></tr></thead><tbody><tr><td style="text-align:center">被检索出(预测结果)</td><td style="text-align:center">True Positive (TP)</td><td style="text-align:center">False Positive (FP)</td></tr><tr><td style="text-align:center">未被检索出(预测结果)</td><td style="text-align:center">False Negative (FN)</td><td style="text-align:center">True Negative (TN)</td></tr></tbody></table><p>其中，TP＋FP＋FN + TN = 样本总数。</p><p>则准确率：<br>$$ P = \frac{TP}{TP+FP}$$</p><p>召回率：<br>$$ R = \frac{TP}{TP + FN}$$</p><p>通常来讲，准确率和召回率是一对矛盾的度量。一般来说，准确率高时，召回率低；召回率高时，准确率偏低。比如一共有m个样本，其中p个正样例，n个负样例。取极限情况，当只检测出一个样本，且它是正样例，则准确率为100%，而召回率为\(\frac{1}{p}\). 当全部样本都被检测出来，召回率为100%, 而准确率为\(\frac{p}{m}\).</p><p>以准确率为纵轴，召回率为横轴，绘制的P-R曲线，常被用作衡量模型优劣的一个标准。如果P-R曲线中，一个曲线\(l_1\)完全包住另一个曲线，则认为\(l_1\)对应的学习器效果好。“平衡点” (Break-Even Point 简称：BEP)为P-R曲线上召回率＝准确率处的点。若模型的BEP点高，说明该模型的准确率和召回率取得相对“双高”的比例，则该模型的效果较好。</p><p>但是BEP还是过于简化了些，更常用的是F1度量：<br>$$F1 = \frac{2\times P \times R}{P + R} = \frac{2\times TP}{m-TN + TP}$$</p><p>F1 是基于准确率和召回率的调和平均 (harmonic mean) 来定义的.<br>$$\frac{1}{F_1} = \frac{1}{2}(\frac{1}{P} + \frac{1}{R})$$.</p><p>然而在一些应用中，对准确率和召回率的重视程度是不同的。例如在商品推荐系统中，为了尽可能少的打扰客户，更希望推荐的内容是客户感兴趣的，这时准确率更重要；在逃犯信息检索系统中，更希望尽可能少的漏掉逃犯，此时召回率更重要。F1的一般形式\(F_{\beta}\)，能让我们表达出对准确率和召回率的不同偏好。<br>$$ F_{\beta} = \frac{(1+ \beta^2)\times P \times R}{(\beta^2 \times P) + R}$$<br>\(\beta = 1\) 时，\(F_{\beta}\) 变为\(F1\); \(\beta &lt; 1\) 时，准确率有更大影响；\(\beta &gt; 1\) 时， 召回率有更大影响。</p><p>当我们有n个混淆矩阵的时候(多分类任务，或者进行多次训练／测试等)，我们如何来求准确率和召回率呢？<br>一种直接的做法是在每个混淆矩阵上分别求出准确率和召回率，然后将这n个准确率和召回率的平均值来作为这n个矩阵的准确率\(\bar P\)、召回率\(\bar R\)，然后用\(\bar P\)和\(\bar R\)算出相应的F1。另一种做法是对n个混淆矩阵的对应元素取平均值，得到\(\bar {TP} \ \bar {FP} \ \bar {TN} \ \bar {FN} \), 在基于这些平均值来计算准确率、召回率和F1.</p><h2 id="R-2-Coefficient-of-Determination"><a href="#R-2-Coefficient-of-Determination" class="headerlink" title="\(R^2\) (Coefficient of Determination)"></a>\(R^2\) (Coefficient of Determination)</h2><p>这学期在上<a href="http://sist.shanghaitech.edu.cn/StaffDetail.asp?id=334" target="_blank" rel="external">王浩</a>老师的机器学习课程，在课上他提到，现在业界很多都通过\(R^2\)来看学习器性能好坏。\(R^2\)衡量的是数据有多大程度上fit我们的model。<br>$$R^2 =  1 - \frac{\sum_{i=1}^{m} (f(\mathbf{x_i}) - y_{i})^2}{\sum_{j=1}^{m} (y_{j} - \bar y)^2} = 1- \frac{SS_{res}}{SS_{tot}}$$.</p><p>\(R^2\)多用于回归问题上，当\(R^2\)越接近1时，说明我们的regression line越完美的fit数据；当\(R^2\)越接近0时，说明regression line 越不fit数据。</p><h2 id="非均等代价"><a href="#非均等代价" class="headerlink" title="非均等代价"></a>非均等代价</h2><p>在现实生活中经常会遇到这种情况，不同类型的错误会造成不同的后果。比如将一个患者诊断为健康人和将一个健康人诊断为患者。后者的影响是增加了进一步检查的麻烦，而前者的后果却可能是失去了拯救生命的最佳时机。为了权衡不同类型错误所造成的不同损失，可以使用非均等代价。 在前面的性能度量中，都隐性的假设了均等代价。而在非均等代价中，我们希望最小化的是“总体代价”。更详细的描述可以参考<a href="http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm" target="_blank" rel="external">周志华老师机器学习新书</a>.</p><h1 id="比较检验："><a href="#比较检验：" class="headerlink" title="比较检验："></a>比较检验：</h1><p>当我们在测试集上得到了学习器的某个性能度量的结果，我们可以用这个数值通过比大小的方式来比较不同学习器的泛化能能力吗？在机器学习中，比较这件事比通常想象的要复杂。对于学习器泛化能力的比较，我们要用<strong>统计假设检验</strong>来说明若在测试集上观察学习器A比学习器B要好，则在统计意义上A的泛化性能是否优于B。更多详细描述可以参考<a href="url1">周志华老师新书</a>.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Book: <a href="prml">Pattern Recognition and Machine Learning by Christopher Bishop</a></li><li><a href="http://www.holehouse.org/mlclass" target="_blank" rel="external">notes of machine learning by Andrew Ng in cousera</a></li><li>GPCA (Generalized Principle Component Analysis) by <a href="http://yima.csl.illinois.edu/" target="_blank" rel="external">Yi Ma</a> (to be published)</li><li><a href="http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm" target="_blank" rel="external">机器学习</a> 周志华</li></ul><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>]]></content>
    
    <summary type="html">
    
      本文介绍了模型评估和选择中用到的一些方法。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/tags/Machine-Learning/"/>
    
      <category term="Model Selection" scheme="http://frankchu0229.github.io/tags/Model-Selection/"/>
    
  </entry>
  
  <entry>
    <title>Statics and Optimization in Machine Learning</title>
    <link href="http://frankchu0229.github.io/2016/03/31/ML2/"/>
    <id>http://frankchu0229.github.io/2016/03/31/ML2/</id>
    <published>2016-03-31T12:50:49.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>Machine Learning 是一门集计算机、统计和优化于一体的学科。这一篇打算写一些机器学习中常用到的统计和优化的基础知识。由于时间有限，英文和中文哪个方便就用哪个写了^_^。</p><h1 id="Jenson’s-Inequality"><a href="#Jenson’s-Inequality" class="headerlink" title="Jenson’s Inequality:"></a>Jenson’s Inequality:</h1><p>Jenson 不等式是机器学习中经常用到的一个不等式。有时我们在evaluate一个变量函数的期望的时候，可能我们不需要知道这个期望的确切数值，只需要知道这个期望的bound就足够了。这时我们就可以用到Jenson不等式。Jenson不等式在推倒EM算法公式的时候会被经常用到。</p><p>令 \(X\)为一随机变量，\(f\)为关于\(X\)的凸函数，则有：<br> $$ E(f(X)) \geq f(E(X))$$<br> 如果 \(f\)为凹函数，则有，<br> $$E(f(X))   \leq f(E(X))$$</p><h1 id="SVD-分解"><a href="#SVD-分解" class="headerlink" title="SVD 分解:"></a>SVD 分解:</h1><p> SVD分解是机器学习中常用到的一种矩阵分解方法。SVD是PCA算法的主要实现手段，而PCA算法又是机器学习中用的最多的降维的方法。</p><p> 矩阵 \(X\) 的 SVD 分解为 \(X ＝ U_X \Sigma_X V_X^T\)，其中 \(X=[x_1,x_2,…x_N] \in R^{D\ \times N}\)，\(U_X \in R^{D\ \times D}\), \(\Sigma_X \in R^{D\ \times N}\), \(V_X \in R^{N\ \times N}\). 其中，\(U_X\), \(V_X\)为unitary matrix (\(U U^{* } = U^{\ * } U = I \))，\(\Sigma_X\)为奇异值矩阵。</p><p> <strong>PCA via SVD:</strong> 令 \(X=[x_1,x_2,…x_N] \in R^{D \times N}\)为减去均值后的数据集，\(X\)的SVD分解结果为 \(X ＝ U_X \Sigma_X V_X^T\), 则经过降维后的数据集为\(\Sigma_X V_X^T\)的顶部\(d\times N\)部分；降维转换矩阵\(U\)为\(U_X\)的前d列。</p><p> ps:关于PCA的推倒和实现会在后续文章里提到。暂时打算将降维和维度诅咒写在一起。</p><h1 id="Matrix-Calculus"><a href="#Matrix-Calculus" class="headerlink" title="Matrix Calculus:"></a>Matrix Calculus:</h1><p> 梯度下降法是机器学习中用的最多的算法之一，而它的关键就是对矩阵求导。求导在 deep learning 里面也是关键。然而矩阵求导一直是我的弱项，复杂一点的有时候要求好久，下面好好梳理下矩阵求导。</p><ul><li>若 \(\mathbf{x}\) 为 \(n\) 维向量，\(f：R^{n} \rightarrow R^{m}  \) 为关于 \(\mathbf{x}\) 的函数，则 \((\frac{\partial f}{\partial \mathbf{x}})_{ij} = \frac{\partial f_i }{\partial x_j}\). </li></ul><ul><li><p>若 \(\mathbf{x}\) 为 \(n\) 维列向量，\(f：R^{n} \rightarrow R \) 为关于 \(\mathbf{x}\) 的函数，则 \(\frac{\partial f}{\partial \mathbf{x}}\) 为\(n\) 维行向量，且\(\frac{\partial f}{\partial \mathbf{x}} = [\frac{\partial f}{\partial x_{i}}]\). </p></li><li><p>若 \(\mathbf{X}\) 为 \(m  \times n\) 的矩阵， \(f：R^{m\ \times n} \rightarrow R \) 为关于 \(\mathbf{X}\) 的函数，则 \(\frac{\partial f}{\partial \mathbf{X}}\) 为\(n \times m\) 的矩阵，且\([\frac{\partial{f}}{\partial \mathbf{X}}]_{ij} = [\frac{\partial f}{X_{ji}}]\).</p></li></ul><p><strong>矩阵求导的三步：</strong></p><ul><li>1  弄清求导的维度</li><li>2  根据求导的维度，将 \(f\) 展开成关于 \(x_i\) 的形式，然后用element-wise calculations 算出每一项的导数</li><li>3  将算出来的每一项导数放到向量或者矩阵里面。</li></ul><h2 id="Example-1-scalar-by-vector"><a href="#Example-1-scalar-by-vector" class="headerlink" title="Example 1: scalar by vector"></a>Example 1: scalar by vector</h2><p> \( \frac{\partial (\mathbf{x}^{T}\mathbf{a})}{\partial \mathbf{x}} =  \frac{\partial (\mathbf{a}^{T} \mathbf{x})}{\partial \mathbf{x}} = \mathbf{a^{T}}\) .</p><p> 根据矩阵求导三步：1. \(\mathbf{x}^{T}\mathbf{a}\) 是两个向量的内机，为scalar；\( \mathbf{x}\)是向量，所以 \(\frac{\partial f}{\partial \mathbf{x}} = [\frac{\partial f}{\partial x_{i}}]\) 是一个行向量. 2. \(\mathbf{x}^{T} \mathbf{a} ＝ \sum_{i=1}^{n} a_i x_i\), 所以 \(\frac{\partial f}{\partial x_{i}} = a_i\). 3. 将每一项求导结果放在向量中就得到了结果：\( \mathbf{a^{T}}\).</p><h2 id="Example-2"><a href="#Example-2" class="headerlink" title="Example 2:"></a>Example 2:</h2><p> \( q(x) = || Ax - b||_2^2\), where \(A\) is \(m \times n\), \(x\) is \(n \times  1\), \(b\) is \(m \times 1\), \(\frac{\partial q(x)}{x} =  ?\)</p><p> 首先看几个其他公式：</p><ul><li>\(y = Ax\), \(\frac{\partial y}{\partial x} = A\), \(\frac{\partial y}{\partial z} = A \frac{\partial x}{\partial z}\)</li><li>\(\alpha = y^T Ax\), \(\frac{\partial \alpha}{\partial x} = y^T A\); \(\alpha = x^T A^T y\),\(\frac{\partial \alpha}{\partial y} = x^T A^T\); \( \frac{\partial \alpha}{\partial A} = yx^{T}\);</li><li>\(\alpha = y^T x\), \(\frac{\partial \alpha}{\partial z} = x^T \frac{\partial y}{\partial z} + y^T \frac{\partial x}{\partial z}\); Proof: \(\frac{\partial \alpha}{\partial z} = \frac{\partial \alpha}{\partial y}\frac{\partial y}{\partial z}+ \frac{\partial \alpha }{\partial x}\frac{\partial x}{\partial z} = x^T \frac{\partial y}{\partial z} + y^T \frac{\partial x}{\partial z} \)</li><li>\(\alpha = x^Tx\),\(\frac{\partial \alpha}{\partial x} = 2x^T\); \(\alpha = x^TAx\), \(\frac{\partial \alpha}{\partial x} = (Ax)^T + x^TA = x^T(A^T + A)\)</li></ul><p>回到我们这个问题上，\( q(x) = || Ax - b||_2^2 ＝ (Ax-b)^T (Ax-b)\), 则从上面公式可得：\(\frac{\partial q(x)}{\partial x} = 2(Ax-b)^T \frac{\partial (Ax-b)}{\partial x} = 2(Ax-b)^TA\).</p><p> \(f(x)\) 关于\(x\) 的二阶导数称为Hessian matrix：\([\frac{\partial^2f(x)}{\partial x^2}]_{ij} = \frac{\partial^2f(x)}{\partial x_i \partial x_j}\), 其中，\(\mathbf{x}\) 为 \(n\) 维列向量，\(f：R^{n} \rightarrow R \) 为关于 \(\mathbf{x}\) 的函数。</p><p> ps：更多关于矩阵求导相关公式会在深度学习中提到。</p><h1 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization:"></a>Optimization:</h1><p> 刚看了周志华老师<a href="url1">新书</a>的附录部分，发现我想写的里面都有，而且深入浅出，浅显易懂^_^。关于优化部分，主要想写下拉格朗日乘子法、KKT条件和一些其他的凸优化相关知识。</p><h2 id="拉格朗日乘子法："><a href="#拉格朗日乘子法：" class="headerlink" title="拉格朗日乘子法："></a>拉格朗日乘子法：</h2><p> 拉格朗日乘子法是寻找多元函数在一组约束条件下极值的方法；通过引入拉格朗日乘子，将d个变量在k个约束条件下的极值问题转化成d＋k个变量无约束的极值问题。<a href="prml">PRML</a>这本书的附录E里面有更加详细的关于拉格朗日乘子的介绍。</p><h3 id="等式约束："><a href="#等式约束：" class="headerlink" title="等式约束："></a>等式约束：</h3><p> 假设 \(\mathbf{x}\) 为 d 维向量，我们要找到一个取值\(\mathbf{x^{*}}\), 使得目标函数 \(f(\mathbf{x})\) 在约束条件\(g(\mathbf{x})=0\) 下取得最小值。 满足约束条件\(g(\mathbf{x})=0\) 的点在一个 \(d-1\) 维的曲面上，对于约束曲面上的任意一点\(x_i\)，它的梯度 \(\nabla g(\mathbf{x_i})\) 都正交于该曲面。对于我们要找的 \(\mathbf{x^{*}}\), 目标函数在该点的梯度 \(\nabla f(\mathbf{x^*})\) 也要正交于约束曲面。因为如果不正交的话，我们可沿着梯度方向在约束曲面上移动一小段距离，从而可以找到更小的值。也就是\(\nabla g(\mathbf{x}) + \lambda \nabla f(\mathbf{x}) = 0\), 其中，\(\lambda \neq 0\)。</p><p> 下面引出拉格朗日函数：\(L(\mathbf{x},\lambda) = f(\mathbf{x}) + \lambda g(\mathbf{x})\). 将 \(L(\mathbf{x},\lambda)\) 对 \(\mathbf{x}\) 求导可得：\(\nabla f(\mathbf{x}) + \lambda \nabla g(\mathbf{x}) = 0\)； 对\(\lambda \) 求导可得：\(g(\mathbf{x})=0\). 所以对拉格朗日函数的无约束优化即等价于原等式约束优化。</p><h3 id="不等式约束："><a href="#不等式约束：" class="headerlink" title="不等式约束："></a>不等式约束：</h3><p>当\(g(\mathbf{x}) \leq 0\), 有两种情况：</p><ul><li>\(g(\mathbf{x}) &lt; 0\) 时, 约束条件 \(g(\mathbf{x}) \leq 0\) 不起作用，此时等价于 \(\lambda ＝ 0\)，可直接通过 \(\nabla f(\mathbf{x}) = 0\) 求解.</li><li>\(g(\mathbf{x}) = 0\) 时，情况和上一条等价约束比较类似，不过此处 \( \lambda&gt; 0\).</li></ul><p>由此引出<strong>KKT条件：</strong> </p><ul><li>\(\nabla g(\mathbf{x}) + \lambda \nabla f(\mathbf{x}) = 0\)</li><li>\(g(\mathbf{x}) \leq 0\)</li><li>\( \lambda \geq 0\)</li><li>\( \lambda g(\mathbf{x}) = 0 \)</li></ul><p>综上，当有多个约束时，考虑有 m 个等式约束和 n 个不等式约束,<br>$$\min_{\mathbf{x}} f(\mathbf{x}) \\ s.t. \ h_{i}(\mathbf{x})=0 \ (i = 1,2,…m) \\ g_{j}(\mathbf{x}) \leq 0 \ (j = 1,2,…n) $$</p><p>引入拉格朗日乘子，\(\mathbf{\lambda} = (\lambda_{1},\lambda_{2},\ldots,\lambda_{m})^{T}\), \(\mathbf{\mu} = (\mu_{1}, \mu_{2},\ldots,\mu_{n})^{T} \), 得到拉格朗日函数为：</p><p>\(L(\mathbf{x},\mathbf{\lambda},\mathbf{\mu}) = f(\mathbf{x}) + \sum_{i=1}^{m}\lambda_{i} h_{i}(\mathbf{x}) + \sum_{j=1}^{n} \mu_{j} g_{j}(\mathbf{x})\)</p><p>其中，由不等式引入的 KKT 条件为 (\(j = 1,2,\ldots,n\))：</p><ul><li>\( g_{j}(\mathbf{x}) \leq 0  \)</li><li>\(\mu_j \geq 0\)</li><li>\(\mu_j g_j(\mathbf{x}) = 0\)</li></ul><h2 id="Convexity-Optimization"><a href="#Convexity-Optimization" class="headerlink" title="Convexity Optimization"></a>Convexity Optimization</h2><p>通过上面拉格朗日乘子和KKT条件算出的结果只是必要条件，只有当目标函数是凸的情况下，才能保证是充分必要条件 (取到全局最优)。通常来讲，找一个函数的全局最优解是很难的；但是如果我们可以将一个问题formulate 成凸优化问题，解决起来会容易很多。</p><h3 id="Convex-Sets"><a href="#Convex-Sets" class="headerlink" title="Convex Sets"></a>Convex Sets</h3><p>在判断一个函数是不是凸函数的时候，首先要看它的定义域是不是一个凸集；所以首先介绍下什么是凸集。</p><p><strong>Definition:</strong> A set \(C \) is convex, if for all \(x,y \ in \ C \) and \(\theta \in R\) with \(0 \leq \theta \leq 1\), \(\theta x + (1-\theta)y \in C \). 意思就是说，对于集合中任意两个元素，我们在这两个元素之间做一条线段，如果线段上的所有点都在集合内，则这个集合就是凸的。</p><p>常见的凸集有：</p><ul><li>All of \(R^{n}\)</li><li>The non-negative orthant, \(R_{+}^{n}\): \(R_{+}^{n} = \{x: x_i \geq 0 \ \forall i= 1,2,…n \}\)</li><li>Norm balls: e.g., \(||x|| \leq 1\)</li><li>Affine subspaces \(\{x \in R^{n} : Ax = b\}\) and polyhedra \(\{x \in R^{n} : Ax \preceq b\}\), where \(\preceq \) denotes componentwise inequality.</li><li>Intersections of convex sets</li><li>Positive semidefine matrices \(S_{+}^{n}\): \(A = A^{T},\) and for all \(x \in R^{n}\), \(x^{T}Ax \geq 0\).</li></ul><h3 id="Convex-Functions"><a href="#Convex-Functions" class="headerlink" title="Convex Functions:"></a>Convex Functions:</h3><p>凸函数是凸优化中一个比较核心的概念，被用在凸优化各个方面。</p><p><strong>Definition:</strong> A function \(R^{n} \rightarrow R \) is convex, if its domain (denoted as D(f)) is a convex set, and if for all \(x,y \in D(f)\) and \(\theta \in R \) with \(0 \leq \theta \leq 1\), \(f(\theta x + (1-\theta)y) \leq \theta f(x) + (1-\theta)f(y)\). </p><p>如果该等式左侧在该条件下(且\(x \neq y\))恒小于右侧，我们就说该函数是 strictly convex。 如果一个函数 \(f \)的负数 \(-f \) 是凸函数，则该函数 \(f\)为凹函数。</p><p>如果函数 \(f \) 是凸函数，则它的局部最小值即为它的全局最小值。如果 \(f \) 是strictly convex的，\(f \) 最多只有一个全局最小值；也就是说如果\(f \)有最小值，那么这个最小值就是唯一的。</p><p><strong>First Order Condition of Convexity:</strong><br>Suppose a function \(R^{n} \rightarrow R \) is differentiable (i.e., the gradient \(\nabla f(x)\) exists for all \(x\) in the domain of \(f\)). Then \(f \) is convex, if and only if \(D(f)\) is a convex set and for all \(x, y \in D(f)\), \(f(y) \geq f(x) + \nabla f(x)^{T}(y-x)\). 也就是说我们在点\(x\)处做切线，用这个切线上的点来近似函数\(f\)上的点，那么这个切线上的所有点都在函数\(f\)相应点的下方。如果恒大于的话，即为strictly convex；如果不等号相反，那么\(f\)即为凹函数。</p><p><strong>Second Order Condition of Convexity:</strong><br> Suppose a function \(R^{n} \rightarrow R \) is twice differentiable (i.e., the Hessian Matrix \(\nabla^{2} f(x)\) is defined for all \(x\) in the domain of \(f\)). Then \(f \) is convex, if and only if \(D(f)\) is a convex set and \(\nabla^{2} f(x)\) is positive semidefine. If \(\nabla^{2} f(x)\) is positive define, then \(f\) is strictly convex. If \(\nabla^{2} f(x)\) is negative semidefine, then \(f\) is concave. 当\(x\)为一维的时候，即为\(\nabla^{2} f(x) \geq 0\)</p><p>If \(f\) is convex, then the \(\alpha\)-sublevel set (\(x \in D(f): f(x) \leq \alpha\)) is a convex set.</p><p><strong>常见的凸函数</strong></p><ul><li>Exponential functions</li><li>Negative logarithm:</li><li>Affine functions: \(f(x) = b^{T}x + c\), \(b \in R^{n}, \ c \in R\). In this case \(\nabla^{2} f(x) = 0\), the affine functions of this form are the \(\mathbf{only}\) functions that are both convex and concave.</li><li>Quadratic functions: \(f(x) = x^{T}Ax + b^{T}x + c \), when \(A\) is symmetric and \(A\) is positive semidefine, \(f(x)\) is convex. ( \(\nabla^{2} f(x) = A\)).</li><li>Norms (using the definition to prove it)</li><li>Nonnegative weighted sums of convex functions</li></ul><h3 id="Convex-Optimization-Problems"><a href="#Convex-Optimization-Problems" class="headerlink" title="Convex Optimization Problems:"></a>Convex Optimization Problems:</h3><p>$$\min_x f(x) \\ s.t. g_i(x)\leq 0, \ i= 1,2,…n \\ h_j(x) = 0,\ j = 1,2,…n $$<br>where \(f\) is a convex function, \(g_i\) are convex functions and \(h_i\) are affine functions.</p><p><strong>Special cases of convex problems:</strong></p><ul><li>Linear Programming (LP):<br>$$\min_x c^{T}x + d \\ s.t. Gx \preceq h \\ Ax = b$$<br>where \(\preceq \) denotes elementwise inequality.</li><li>Quadratic Programming (QP):<br>$$ \min_x \frac{1}{2} x^{T}Px + c^{T}x + d \\ s.t. Gx \preceq h \\ Ax = b$$ where \(P\) is a symmetric positive semidefine matrix.</li><li>Quadratically Constrained Quadratic Programming (QCQP):<br>$$\min_x \frac{1}{2} x^{T}Px + c^{T} +d \\ s.t. \frac{1}{2} x^{T}Q_ix + r_i^{T}x + s_i \leq 0 \ \ i = 1,2…,m \\ Ax= b$$<br>where \(P\) and \(Q_i\) are symmetric positive semidefine matrices. </li></ul><h1 id="共轭分布、多项分布与-Dirichlet-分布"><a href="#共轭分布、多项分布与-Dirichlet-分布" class="headerlink" title="共轭分布、多项分布与 Dirichlet 分布"></a>共轭分布、多项分布与 Dirichlet 分布</h1><p>在很多模型中 (e.g., LDA)，一个节点到其他节点的概率之和为1，这时我们就用多项分布来建模。比如现在我们有一个\(d\)维向量\(\mathbf{x}\), 其中\(\mathbf{x}\)是 1-of-K 的形式。令\(x_i\)取1 的概率为\(\mu_i\), 并且\(\sum_{i = 1}^{d}\mu_i = 1\),那么我们就能得到关于\(\mathbf{x}\)的多项分布：\(P(\mathbf{x}|\mathbf{\mu}) = \prod_{i=1}^{d} \mu_i^{x_i}\). 其中，\(\mathbf{\mu}\)即为我们模型的参数。</p><p>现在我们想对参数\(\mathbf{\mu}\)加先验，我们认为参数\(\mathbf{\mu}\)服从一个分布。通常来讲，我们都会选共轭分布来作为先验分布 (在很多情况下，共轭分布能使问题简化)。多项分布的共轭分布是Dirichlet 分布，因为将多项分布和Dirichlet 分布相乘仍是Dirichlet 分布。在生成模型中，多项分布和Dirichlet 分布用的比较多，这里先简单介绍下，在后面的LDA模型中，我会详细介绍。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Book: <a href="prml">Pattern Recognition and Machine Learning by Christopher Bishop</a></li><li><a href="http://www.holehouse.org/mlclass" target="_blank" rel="external">notes of machine learning by Andrew Ng in cousera</a></li><li>GPCA (Generalized Principle Component Analysis) by <a href="http://yima.csl.illinois.edu/" target="_blank" rel="external">Yi Ma</a> (to be published)</li><li><a href="http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm" target="_blank" rel="external">机器学习</a> 周志华</li><li><a href="MD">Matrix Differentiation</a></li><li><a href="MCB">Matrix CookBook</a></li><li><a href="http://cs229.stanford.edu/" target="_blank" rel="external">Notes of CS229 in Stanford</a></li></ul><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>]]></content>
    
    <summary type="html">
    
      本文介绍了机器学习中常用到的统计和优化的基础知识。
    
    </summary>
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/tags/Machine-Learning/"/>
    
      <category term="SVD" scheme="http://frankchu0229.github.io/tags/SVD/"/>
    
      <category term="Jesen&#39;s Inequality" scheme="http://frankchu0229.github.io/tags/Jesen-s-Inequality/"/>
    
      <category term="Matrix Calculus" scheme="http://frankchu0229.github.io/tags/Matrix-Calculus/"/>
    
      <category term="Optimization" scheme="http://frankchu0229.github.io/tags/Optimization/"/>
    
  </entry>
  
  <entry>
    <title>Introduction</title>
    <link href="http://frankchu0229.github.io/2016/03/19/ML1/"/>
    <id>http://frankchu0229.github.io/2016/03/19/ML1/</id>
    <published>2016-03-19T08:15:23.000Z</published>
    <updated>2017-10-30T01:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>接触machine learning已经有一年多的时间了，在这段时间里，经历了从不懂到了解，从了解到理解，从理解到运用的几个阶段。起初开始学习maching learning的时候，一般以看为主，很少自己推倒，这样过了一段时间之后，发现之前看的算法基本都忘了。后来上<a href="https://scholar.google.com/citations?user=XqLiBQMAAAAJ" target="_blank" rel="external">马毅</a>老师GPCA课的时候，他曾告诫我们，要想把知识学好并变为自己的东西，最好的办法就是自己从头到尾推倒一遍. 在做数据科学国际大会<a href="http://ssds2015.shanghaitech.edu.cn/" target="_blank" rel="external">SSDS2015</a>志愿者的时候，有幸负责接送<a href="http://www.cs.cmu.edu/~epxing/" target="_blank" rel="external">Eric Xing</a>教授，在路上和他聊天的时候，Eric也指出学习maching learning一定要自己推倒，这是最基本的要求。在这之后的学习中，对所遇到的算法，都要自己推倒一番，发现效果很不错。</p><p>一直想整理machine learning方面的知识写成博客，但总是找不到太合适的时间。最近学院学分政策变了，我选了学院machine learning的课，加上最近在读周志华老师机器学习的<a href="http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm" target="_blank" rel="external">新书</a>，便决定把之前学习的maching learning知识好好梳理一下。</p><h2 id="What-is-machine-Learning"><a href="#What-is-machine-Learning" class="headerlink" title="What is machine Learning?"></a>What is machine Learning?</h2><p>Arthur Samuel (1959): “Field of study that gives computers the ability to learn without being explicitly programmed.”</p><p>Tom Michel (1999): “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”</p><p>My understanding：machine learning gives the computers the ability to learn, and according to different tasks, the computers learn different things in order to improve the performance. The machine learning algorithms can solve problems better than the algorithms used in the old days. </p><p><strong>Exapmle: Recognizing handwritten digits.</strong> </p><p>Recognizing handwritten digits is not a nontrival problem due to the wide variability of handwritting. It can be tackled using handcrafted rules for distinguishing the digits based on the shapes of the strokes, but in practice such an approach leads to a proliferation of rules and of excpetions to the rules and so on, and invariably gives poor performance. However, using the machine learning methods can get a much better performance. <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="external">Example</a>. In recent research, the use of deeplearning methods can reduce the error rate of recognizing handwritten digits down to less than 0.5%. </p><h2 id="Why-machine-learning"><a href="#Why-machine-learning" class="headerlink" title="Why machine learning?"></a>Why machine learning?</h2><p>机器学习可以比较好的解决传统编程不能解决好的问题。就拿手写体识别这个例子来说，如果用传统编程的方式，我们很难设计手写体的全部规则并加以编程实现。但如果把这个问题用机器学习的算法来解决，实现起来比较简单，效果也好。</p><h2 id="Inductive-learning"><a href="#Inductive-learning" class="headerlink" title="Inductive learning"></a>Inductive learning</h2><p>机器学习－“从样例中学习”是一个归纳过程，所以也称为归纳学习。通常我们假设整个样本空间(由特征张成的空间)服从一个未知的分布D，每个样本都是从这个分布上独立同分布的采样得到的。我们获得的样本越多，对样本空间的信息知道的就越多。训练集只是样本空间的一个很小的采样，但是我们希望它能够很好的反映样本空间的特性，否则在训练集上学到的模型范化能力会比较差。</p><p>我们可以把模型的学习过程看成是一个在由所有假设组成的空间中进行搜索的过程，搜索目标是与训练集相匹配的假设。但是在实际问题中，假设空间会很大，可能会得到很多个和训练集相匹配的假设(版本空间)。如何在版本空间中选择我们需要的模型？一种选择标准是“奥卡姆剃刀”。“奥卡姆剃刀”是一种常用的，自然科学研究中最基本的原则，即“若有多个假设与观察一致，选择最简单的那个”。但是在模型选择上，通常不好定义什么是“简单”。 另一个标准是归纳偏好(inductive bias)，即对某种类型假设的偏好。</p><h2 id="No-Free-Lunch-Theorem-NFL"><a href="#No-Free-Lunch-Theorem-NFL" class="headerlink" title="No Free Lunch Theorem (NFL)"></a>No Free Lunch Theorem (NFL)</h2><p>NFL(没有免费的午餐定理)是说，无论算法a多聪明，算法b多笨拙，他们的期望性能是相同的。NFL有一个重要前提：所有问题同等重要。但有的时候我们只关心自己试图正在解决的问题，并不关心这个方案在其他问题上的效果。</p><p>NFL让我们认识到，脱离具体问题去空谈“什么学习算法好”毫无意义，因为若要考虑所有问题，所有算法都一样好。所以若要讨论算法的相对优劣，必须针对具体问题。在有些问题上表现比较好的算法，在其他问题上却可能不尽人意。</p><h2 id="Some-concepts"><a href="#Some-concepts" class="headerlink" title="Some concepts"></a>Some concepts</h2><p><strong>Supervised Learning</strong></p><ul><li>Teach the computer how to do something, then let it use it;</li><li>the data has labels</li><li>example: linear regression，classification</li></ul><p><strong>Unsupervised Learning</strong></p><ul><li>Let the computer learn how to do something, and use this to determine structure and patterns in data</li><li>the data has no labels</li><li>example : clustering</li></ul><p>For more detailed examples for unsupervised and supervised learning, you can see <a href="http://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html" target="_blank" rel="external">this</a>.</p><hr><p><strong>Generative Models</strong></p><ul><li><p>Generative Models are used to model how data are generated. </p></li><li><p>For example, you are given a dataset and you plot the instances in the dataset. From the plot, you find the distribution look like Gaussian distribution. Then you choose Gaussian distribution as your model. Now, you think your dataset are genereatd from this Gaussian distribution. For each instance, there is a generating probability \(p_i\), then the likelihood of generating the whole dataset is \(\prod_{i=1}^{N}p_i\), so we can learn the parameters by maxmizing the likelihood.</p></li></ul><p>What if we cannot plot the dataset? In this case what kind of models should we use? Model selection can solve this problem well and it will be introduced in the following part.</p><ul><li><p>Another view : Generative Model: Modeling the joint distribution of all data. If the goal is to learn f: X –&gt; Y, e.g., P(Y|X), generative models estimate parameters of P(X|Y),p(Y) directly from training data. Then we can use Bayes rule to calculate P(Y|X=x). Usually P(X|Y) can be related to the probability of generating the data given the model.<br>(P(Y|X) = \(\frac{P(X|Y)P(Y)}{P(X)}\))</p></li><li><p>Example: Mixtures of Gaussians</p></li></ul><p><strong>Discriminative Models</strong></p><ul><li><p>Directly assume some functional form for P(Y|X)</p></li><li><p>Estimate P(Y|X) directly from training data.</p></li><li><p>There is a good saying for discriminative models: discriminative models don’t do anything more than they are asked to do.</p></li><li><p>Examples: SVM, Logistic Regression, Neural Network</p></li></ul><hr><h2 id="Model-Selection"><a href="#Model-Selection" class="headerlink" title="Model Selection"></a>Model Selection</h2><p>In order to choose the best model, we need to do model selection. If we have plentiful data, then one approach is to use some of the data to train some models, and then to compare them on independent data(validation set) and select the one having the best performance.</p><p>What if the supply of data is limited? We know that, in order to build good models, we wish to use as much of the available data as possible. So in this situation, we need to use <strong>K-fold cross-validation</strong>:</p><ul><li><p>Divide the dataset into K groups</p></li><li><p>Take one group as test set, the others as training set.</p></li><li><p>Repeat the last step, but this time choose another group as the test set. </p></li><li><p>Repeat like step 3 until all groups are used for test set.</p></li><li><p>calculate the average performance tested from the above test sets.</p></li><li><p>choose the best one</p></li></ul><p><strong>Drwabacks:</strong><br>The number of training runs in cross-validation is K times comparing with no cross-validation. This can be problematic for models in which the training costs a lot of time.</p><h2 id="The-Curse-of-Dimensionality"><a href="#The-Curse-of-Dimensionality" class="headerlink" title="The Curse of Dimensionality"></a>The Curse of Dimensionality</h2><p>If the dimension of the data is too high, we will meet the following problems:</p><ul><li><p>The complexity of the models will increase as the dimensionality of the dataset increases.</p></li><li><p>Our geometric intuitions can fail badly when we consider spaces of higher dimensionality. In spaces of high dimensionality, most of the volume of a sphere is concentrated in a thin shell near the surface.</p></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li>Book: <a href="http://www.rmki.kfki.hu/~banmi/elte/Bishop%20-%20Pattern%20Recognition%20and%20Machine%20Learning.pdf" target="_blank" rel="external">Pattern Recognition and Machine Learning by Christopher Bishop</a></li><li><a href="http://www.holehouse.org/mlclass" target="_blank" rel="external">notes of machine learning by Andrew Ng in cousera</a></li><li>GPCA(Generalized Principle Component Analysis) by <a href="http://yima.csl.illinois.edu/" target="_blank" rel="external">Yi Ma</a> (to be published)</li><li>龙星计划2010 slides by Eric Xing</li><li><a href="http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm" target="_blank" rel="external">机器学习</a> 周志华</li></ul><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>]]></content>
    
    <summary type="html">
    
      The reason why I write this series of blogs and some basic concepts of machine learning.
    
    </summary>
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Machine Learning" scheme="http://frankchu0229.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
</feed>
